{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue Inspection - Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyi/miniconda3/envs/sc/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "from functools import partial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# load config\n",
    "cfg = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-cos-v1\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def __dist__(A: np.ndarray, B: np.ndarray, n_segments: int, lamda: float = 0.5) -> float:\n",
    "    # calculate normalized positional distance\n",
    "    if lamda > 0:                                               # include position\n",
    "        A_pos, B_pos = int(A[0]), int(B[0])\n",
    "        pos_dist = abs(A_pos - B_pos)                           # integer [1, n_segments]\n",
    "        pos_dist_norm = pos_dist / n_segments                   # float [0, 1]\n",
    "        A_vec, B_vec = A[1:], B[1:]\n",
    "    else:                                                       # exclude position\n",
    "        pos_dist_norm = 0\n",
    "        A_vec, B_vec = A, B\n",
    "\n",
    "    # calculate normalized cosine distance\n",
    "    cos_sim = np.dot(A_vec, B_vec) / (np.linalg.norm(A_vec) * np.linalg.norm(B_vec))\n",
    "    cos_dist = 1 - cos_sim                                      # float [0, 2]\n",
    "    cos_dist_norm = cos_dist / 2                                # float [0, 1]\n",
    "\n",
    "    # calculate weighted average\n",
    "    return lamda * pos_dist_norm + (1-lamda) * cos_dist_norm    # float [0, 1]\n",
    "\n",
    "def split_sentences(doc: str):\n",
    "    return [text.text.strip() for segment in doc.split(\"\\n\") for text in nlp(segment).sents]\n",
    "\n",
    "def group_by_cluster(texts, labels):\n",
    "    df = pd.DataFrame({\"label\": labels, \"texts\": texts})\n",
    "    df = df.groupby(\"label\")[\"texts\"].apply(lambda x: '\\n'.join(x)).reset_index()\n",
    "    return df[\"texts\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "def log_clusters(texts, ids):\n",
    "    for text, id in zip(texts, ids):\n",
    "        print(id, text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use \"Attention is All You Need\" paper as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_attention = \"\"\"Attention Is All You Need\n",
    "\n",
    "Abstract\n",
    "\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
    "\n",
    "1 Introduction\n",
    "\n",
    "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
    "\n",
    "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
    "\n",
    "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
    "\n",
    "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
    "\n",
    "2 Background\n",
    "\n",
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
    "\n",
    "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
    "\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
    "\n",
    "3 Model Architecture\n",
    "\n",
    "Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations. Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
    "\n",
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
    "\n",
    "Encoder and Decoder Stacks\n",
    "\n",
    "The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
    "\n",
    "The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "\n",
    "Attention\n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "Scaled Dot-Product Attention\n",
    "\n",
    "The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n",
    "\n",
    "Attention(Q, K, V) = softmax(QK^T / sqrt(dk)) V\n",
    "\n",
    "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk). Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively. On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work, we employ h = 8 parallel attention layers, or heads. For each of these, we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "\n",
    "Applications of Attention in our Model\n",
    "\n",
    "The Transformer uses multi-head attention in three different ways:\n",
    "\n",
    "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
    "\n",
    "- The encoder contains self-attention layers. In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
    "\n",
    "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
    "\n",
    "Position-wise Feed-Forward Networks\n",
    "\n",
    "In addition to attention sub\n",
    "\n",
    "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "Embeddings and Softmax\n",
    "\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to. In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
    "\n",
    "Positional Encoding\n",
    "\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this work, we use sine and cosine functions of different frequencies.\n",
    "\n",
    "4 Why Self-Attention\n",
    "\n",
    "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
    "\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
    "\n",
    "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
    "\n",
    "5 Training\n",
    "\n",
    "Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
    "\n",
    "Hardware and Schedule\n",
    "\n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
    "\n",
    "Optimizer\n",
    "\n",
    "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9. We varied the learning rate over the course of training.\n",
    "\n",
    "Regularization\n",
    "\n",
    "We employ three types of regularization during training:\n",
    "\n",
    "- Residual Dropout: We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
    "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
    "\n",
    "6 Results\n",
    "\n",
    "Machine Translation\n",
    "\n",
    "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
    "\n",
    "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
    "\n",
    "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
    "\n",
    "Model Variations\n",
    "\n",
    "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging.\n",
    "\n",
    "English Constituency Parsing\n",
    "\n",
    "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
    "\n",
    "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
    "\n",
    "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
    "\n",
    "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
    "\n",
    "7 Conclusion\n",
    "\n",
    "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
    "\n",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task, our best model outperforms even all previously reported ensembles.\n",
    "\n",
    "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video. Making generation less sequential is another research goal of ours.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the implementation for the old `k-split` cluster chunker. Note that the implementation only uses the default functions from `scipy` except the distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_linkage_clustering_k_split(data: np.ndarray, lamda: float = 0., n_clusters: int = 10):\n",
    "    metric = partial(__dist__, n_segments=len(data), lamda=lamda)\n",
    "    distance_matrix = np.abs(pdist(data, metric=metric))\n",
    "    linkage_matrix = linkage(distance_matrix, method='single')\n",
    "    cluster_labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
    "    return cluster_labels\n",
    "\n",
    "def cluster_chunker_k_split(doc: str, doc_id: str, lamda: float = 0., n_clusters: int = 10):\n",
    "    sents = split_sentences(doc)\n",
    "    embs = model.encode(sents)\n",
    "    if lamda > 0: embs = np.insert(embs, 0, range(len(embs)), axis=1)     # insert position in front of semantic embeddings\n",
    "    labels = single_linkage_clustering_k_split(embs, n_clusters=n_clusters)\n",
    "    texts, ids = group_by_cluster(sents, labels)\n",
    "    ids = [\"{}|{}\".format(doc_id, cluster_id) for cluster_id in ids]\n",
    "    return texts, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result of running scipy-based cluster chunker on \"Attention is All You Need\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Attention Is All You Need\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter.\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n",
      "In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions.\n",
      "In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
      "Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time.\n",
      "At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "Encoder and Decoder Stacks\n",
      "The encoder is composed of a stack of N = 6 identical layers.\n",
      "Each layer has two sub-layers.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
      "We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "Scaled Dot-Product Attention\n",
      "The input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The keys and values are also packed together into matrices K and V.\n",
      "We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\n",
      "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk).\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n",
      "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
      "To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively.\n",
      "On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
      "These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "In this work, we employ h = 8 parallel attention layers, or heads.\n",
      "For each of these, we use dk = dv = dmodel/h = 64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
      "- The encoder contains self-attention layers.\n",
      "In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "Position-wise Feed-Forward Networks\n",
      "In addition to attention sub\n",
      "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.\n",
      "In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed.\n",
      "4 Why Self-Attention\n",
      "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "We plan to investigate this approach further in future work.\n",
      "5 Training\n",
      "Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models, step time was 1.0 seconds.\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "Optimizer\n",
      "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9.\n",
      "We varied the learning rate over the course of training.\n",
      "Regularization\n",
      "We employ three types of regularization during training:\n",
      "- Residual Dropout:\n",
      "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "For the base model, we use a rate of Pdrop = 0.1.\n",
      "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1.\n",
      "Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "The configuration of this model is listed in the bottom line of Table 3.\n",
      "Training took 3.5 days on 8 P100 GPUs.\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
      "Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences.\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences.\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
      "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task, our best model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n",
      "Making generation less sequential is another research goal of ours.\n",
      "\n",
      "doc0|2 This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "doc0|3 This would increase the maximum path length to O(n/r).\n",
      "\n",
      "doc0|4 These hyperparameters were chosen after experimentation on the development set.\n",
      "\n",
      "doc0|5 2 Background\n",
      "\n",
      "doc0|6 Hardware and Schedule\n",
      "\n",
      "doc0|7 6 Results\n",
      "7 Conclusion\n",
      "\n",
      "doc0|8 Abstract\n",
      "1 Introduction\n",
      "\n",
      "doc0|9 In this work, we use sine and cosine functions of different frequencies.\n",
      "\n",
      "doc0|10 V\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_split(doc_attention, doc_id=\"doc0\", n_clusters=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the chunk `doc0|1` is far too long, whereas other chunks are far too short, almost only a single sentence.\n",
    "\n",
    "Before diving into the \"why\", let's first take a look at `k-preserve` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_linkage_clustering_k_preserve(data: np.ndarray, lamda: float = 0., min_samples: Optional[int] = None):\n",
    "    if min_samples is None:\n",
    "        min_samples = len(data) // 10 + 1\n",
    "    if min_samples > len(data)/2:\n",
    "        min_samples = len(data) // 10 + 1\n",
    "    n_clusters = len(data)//min_samples      # k-preserve initializes with total number of segments as n_clusters\n",
    "\n",
    "    # perform single-linkage clustering with n_clusters first\n",
    "    metric = partial(__dist__, n_segments=len(data), lamda=lamda)\n",
    "    distance_matrix = np.abs(pdist(data, metric=metric))\n",
    "    linkage_matrix = linkage(distance_matrix, method='single')\n",
    "    cluster_labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "    while True:\n",
    "        # find the small clusters with less than min_samples\n",
    "        unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        small_clusters = unique_labels[counts < min_samples]\n",
    "        small_counts = counts[counts < min_samples]\n",
    "\n",
    "        # if no small clusters, break\n",
    "        if len(small_clusters) == 0:\n",
    "            break\n",
    "\n",
    "        # find the smallest cluster to merge\n",
    "        smallest_cluster = small_clusters[np.argmin(small_counts)]\n",
    "        smallest_indices = np.where(cluster_labels == smallest_cluster)[0]\n",
    "\n",
    "        # find the closest cluster to merge with\n",
    "        min_distance = np.inf\n",
    "        closest_cluster = None\n",
    "        distance_square = squareform(distance_matrix)\n",
    "        for i in smallest_indices:\n",
    "            distances = distance_square[i]\n",
    "            for lbl in unique_labels:\n",
    "                if lbl != smallest_cluster:\n",
    "                    cluster_indices = np.where(cluster_labels == lbl)[0]\n",
    "                    mean_distance = np.mean(distances[cluster_indices])\n",
    "                    if mean_distance < min_distance:\n",
    "                        min_distance = mean_distance\n",
    "                        closest_cluster = lbl\n",
    "\n",
    "        # merge the smallest cluster with the closest cluster\n",
    "        cluster_labels[cluster_labels == smallest_cluster] = closest_cluster\n",
    "\n",
    "    # relabel clusters consecutively\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    new_labels = np.zeros_like(cluster_labels)\n",
    "    for new_label, unique_label in enumerate(unique_labels, start=1):\n",
    "        new_labels[cluster_labels == unique_label] = new_label\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def cluster_chunker_k_preserve(doc: str, doc_id: str, lamda: float = 0., min_samples: Optional[int] = None):\n",
    "    sents = split_sentences(doc)\n",
    "    embs = model.encode(sents)\n",
    "    if lamda > 0: embs = np.insert(embs, 0, range(len(embs)), axis=1)     # insert position in front of semantic embeddings\n",
    "    labels = single_linkage_clustering_k_preserve(embs, lamda=lamda, min_samples=min_samples)\n",
    "    texts, ids = group_by_cluster(sents, labels)\n",
    "    ids = [\"{}|{}\".format(doc_id, cluster_id) for cluster_id in ids]\n",
    "    return texts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Attention Is All You Need\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter.\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n",
      "In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions.\n",
      "In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
      "Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time.\n",
      "At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "Encoder and Decoder Stacks\n",
      "The encoder is composed of a stack of N = 6 identical layers.\n",
      "Each layer has two sub-layers.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
      "We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "Scaled Dot-Product Attention\n",
      "The input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The keys and values are also packed together into matrices K and V.\n",
      "We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\n",
      "V\n",
      "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk).\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n",
      "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
      "To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively.\n",
      "On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
      "These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "In this work, we employ h = 8 parallel attention layers, or heads.\n",
      "For each of these, we use dk = dv = dmodel/h = 64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
      "- The encoder contains self-attention layers.\n",
      "In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "Position-wise Feed-Forward Networks\n",
      "In addition to attention sub\n",
      "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.\n",
      "In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed.\n",
      "In this work, we use sine and cosine functions of different frequencies.\n",
      "4 Why Self-Attention\n",
      "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "This would increase the maximum path length to O(n/r).\n",
      "We plan to investigate this approach further in future work.\n",
      "5 Training\n",
      "Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models, step time was 1.0 seconds.\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "Optimizer\n",
      "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9.\n",
      "We varied the learning rate over the course of training.\n",
      "Regularization\n",
      "We employ three types of regularization during training:\n",
      "- Residual Dropout:\n",
      "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "For the base model, we use a rate of Pdrop = 0.1.\n",
      "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1.\n",
      "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "The configuration of this model is listed in the bottom line of Table 3.\n",
      "Training took 3.5 days on 8 P100 GPUs.\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6.\n",
      "These hyperparameters were chosen after experimentation on the development set.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
      "Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences.\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences.\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
      "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task, our best model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n",
      "Making generation less sequential is another research goal of ours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_preserve(doc_attention, doc_id=\"doc0\")\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when we do not set `min_samples`, the chunk `doc0|1` is the whole document, just like a `whole chunker`.\n",
    "\n",
    "What happens if we set `min_samples`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Attention Is All You Need\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter.\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n",
      "In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions.\n",
      "In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
      "Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time.\n",
      "At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "Encoder and Decoder Stacks\n",
      "The encoder is composed of a stack of N = 6 identical layers.\n",
      "Each layer has two sub-layers.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
      "We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "Scaled Dot-Product Attention\n",
      "The input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The keys and values are also packed together into matrices K and V.\n",
      "We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\n",
      "V\n",
      "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk).\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n",
      "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
      "To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively.\n",
      "On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
      "These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "In this work, we employ h = 8 parallel attention layers, or heads.\n",
      "For each of these, we use dk = dv = dmodel/h = 64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
      "- The encoder contains self-attention layers.\n",
      "In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "Position-wise Feed-Forward Networks\n",
      "In addition to attention sub\n",
      "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.\n",
      "In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed.\n",
      "In this work, we use sine and cosine functions of different frequencies.\n",
      "4 Why Self-Attention\n",
      "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "This would increase the maximum path length to O(n/r).\n",
      "We plan to investigate this approach further in future work.\n",
      "5 Training\n",
      "Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models, step time was 1.0 seconds.\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "Optimizer\n",
      "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9.\n",
      "We varied the learning rate over the course of training.\n",
      "Regularization\n",
      "We employ three types of regularization during training:\n",
      "- Residual Dropout:\n",
      "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "For the base model, we use a rate of Pdrop = 0.1.\n",
      "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1.\n",
      "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "The configuration of this model is listed in the bottom line of Table 3.\n",
      "Training took 3.5 days on 8 P100 GPUs.\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6.\n",
      "These hyperparameters were chosen after experimentation on the development set.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
      "Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences.\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences.\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
      "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task, our best model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n",
      "Making generation less sequential is another research goal of ours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_preserve(doc_attention, doc_id=\"doc0\", min_samples=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing. It's still the same as `whole chunker`.\n",
    "\n",
    "Now. Let's take a look at the new version of single-linkage clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constrained single-linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import get_encoder\n",
    "\n",
    "def get_dist_mat(data: np.ndarray, lamda: float = 0.5) -> np.ndarray:\n",
    "    return np.triu(squareform(np.abs(pdist(data, metric=partial(__dist__, n_segments=len(data), lamda=lamda)))))\n",
    "\n",
    "def get_curr_min_idx(arr: np.ndarray):\n",
    "    return np.argmin(arr)//len(arr), np.argmin(arr)%len(arr)\n",
    "\n",
    "def single_linkage_clustering(embs: np.ndarray, lamda: float = 0.5, max_samples_per_cluster: Optional[int] = None):\n",
    "    # get distance matrix\n",
    "    dist_mat = get_dist_mat(embs, lamda=lamda)\n",
    "    dist_mat[dist_mat == 0] = float('inf')  # set diagonal to infinity so that the minimum value can be iterated\n",
    "\n",
    "    # set max_samples_per_cluster\n",
    "    if not max_samples_per_cluster: max_samples_per_cluster = len(embs) // 10\n",
    "\n",
    "    # initialize clusters and parents\n",
    "    clusters = {i: [i] for i in range(len(embs))}\n",
    "    parents = {i: i for i in range(len(embs))}\n",
    "\n",
    "    # iterate through the distance matrix from min to max for single linkage clustering\n",
    "    while dist_mat.min() < float('inf'):\n",
    "        row, col = get_curr_min_idx(dist_mat)\n",
    "        parent_row, parent_col = parents[row], parents[col]\n",
    "        if parent_row != parent_col and len(clusters[parent_row]) + len(clusters[parent_col]) <= max_samples_per_cluster:\n",
    "            clusters[parent_row].extend(clusters[parent_col])\n",
    "            for sample in clusters[parent_col]:\n",
    "                parents[sample] = parent_row\n",
    "            del clusters[parent_col]\n",
    "        dist_mat[row, col] = float('inf')\n",
    "\n",
    "    # convert parents to cluster labels\n",
    "    parent_to_cluster_label = {}\n",
    "    temp_lbl = 0\n",
    "    for parent in parents.values():\n",
    "        if parent not in parent_to_cluster_label:\n",
    "            parent_to_cluster_label[parent] = temp_lbl\n",
    "            temp_lbl += 1\n",
    "\n",
    "    # get cluster labels for all samples\n",
    "    cluster_labels = [parent_to_cluster_label[parents[idx]] for idx in range(len(embs))]\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "def cluster_chunker(doc, doc_id, encoder = None, lamda: float = 0., max_samples_per_cluster: Optional[int] = None):\n",
    "    if not encoder: raise ValueError(\"Encoder is required for cluster_chunker\")\n",
    "    sents = split_sentences(doc)\n",
    "    embs = encoder.embed_documents(sents)\n",
    "    labels = single_linkage_clustering(embs, lamda=lamda, max_samples_per_cluster=max_samples_per_cluster)\n",
    "    texts, ids = group_by_cluster(sents, labels)\n",
    "    ids = [\"{}|{}\".format(doc_id, cluster_id) for cluster_id in ids]\n",
    "    return texts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|0 Attention Is All You Need\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n",
      "In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively.\n",
      "On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
      "Applications of Attention in our Model\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
      "In addition to attention sub\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "\n",
      "doc0|1 Abstract\n",
      "1 Introduction\n",
      "2 Background\n",
      "3 Model Architecture\n",
      "V\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "This would increase the maximum path length to O(n/r).\n",
      "Hardware and Schedule\n",
      "Optimizer\n",
      "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9.\n",
      "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1.\n",
      "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "The configuration of this model is listed in the bottom line of Table 3.\n",
      "These hyperparameters were chosen after experimentation on the development set.\n",
      "Model Variations\n",
      "7 Conclusion\n",
      "\n",
      "doc0|2 The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
      "Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time.\n",
      "We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "Positional Encoding\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed.\n",
      "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "\n",
      "doc0|3 The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "Encoder and Decoder Stacks\n",
      "The encoder is composed of a stack of N = 6 identical layers.\n",
      "The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "- The encoder contains self-attention layers.\n",
      "In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "\n",
      "doc0|4 Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "4 Why Self-Attention\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\n",
      "English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences.\n",
      "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
      "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n",
      "\n",
      "doc0|5 Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "- Residual Dropout:\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences.\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task, our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "doc0|6 Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter.\n",
      "At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
      "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "The input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
      "The keys and values are also packed together into matrices K and V.\n",
      "We compute the matrix of outputs as:\n",
      "These are concatenated and once again projected, resulting in the final values.\n",
      "For each of these, we use dk = dv = dmodel/h = 64.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.\n",
      "In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "doc0|7 Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions.\n",
      "Each layer has two sub-layers.\n",
      "This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "We plan to investigate this approach further in future work.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "Making generation less sequential is another research goal of ours.\n",
      "\n",
      "doc0|8 In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
      "Scaled Dot-Product Attention\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk).\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n",
      "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
      "To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
      "Multi-Head Attention\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "In this work, we employ h = 8 parallel attention layers, or heads.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "Position-wise Feed-Forward Networks\n",
      "\n",
      "doc0|9 In this work, we use sine and cosine functions of different frequencies.\n",
      "Machine Translation\n",
      "\n",
      "doc0|10 5 Training\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models, step time was 1.0 seconds.\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "We varied the learning rate over the course of training.\n",
      "Regularization\n",
      "We employ three types of regularization during training:\n",
      "For the base model, we use a rate of Pdrop = 0.1.\n",
      "Training took 3.5 days on 8 P100 GPUs.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker(doc_attention, doc_id=\"doc0\", encoder=get_encoder(\"huggingface\"))\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the chunks have similar sample sizes and look much more consistent compared to the old methods.\n",
    "\n",
    "Let's look at another example, randomly taken from the MLDR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Negative Zone is a fictional setting, an antimatter universe appearing in American comic books published by Marvel Comics. The location is depicted in various publications from Marvel, most frequently in Fantastic Four and Captain Marvel. Created by Stan Lee and Jack Kirby, it first appeared in Fantastic Four #51 (June 1966).\n",
      "\n",
      "Fictional description\n",
      "The Negative Zone in the Marvel Universe is used as a fictional dimension. Essentially, it is a universe parallel to Earth's. The two have many similarities, but a few noteworthy differences include: all matter in the Negative Zone is negatively charged; the Negative Zone is entirely filled with a pressurized, breathable atmosphere; and near the center of the Negative Zone is a deadly vortex of unspeakable power. Since the Negative Zone is largely uninhabited, several would-be conquerors have attempted to bridge the gap to Earth and take over its population. A few notable residents of the Negative Zone include Blastaar and Annihilus.\n",
      "\n",
      "The Negative Zone is often visited by the Fantastic Four as Mister Fantastic discovered it, and has mapped portions of it extensively. The Age of Apocalypse version of Blink also visited it once, which proved that there's only one Negative Zone, as Annihilus remembered the Fantastic Four even though they never existed in the Age of Apocalypse.\n",
      "\n",
      "For a number of years, Captain Mar-Vell and Rick Jones were bonded to each other, causing one of them to exist in the Negative Zone while the other would exist in the regular universe. They exchanged places by clasping the special bracelets each wore or automatically after a few hours.\n",
      "\n",
      "Spider-Man has also visited the Negative Zone, and acquired a costume that allowed him to merge with shadows and become practically invisible. When he was framed by Norman Osborn a few weeks later, he used the costume to become the dark, mysterious Dusk (one of his four new superhero identities during the Identity Crisis story arc). A few months later after Spider-Man's name was cleared, Cassie St. Commons was given the guise of Dusk and joined the Slingers. Cletus Kasady also visited the Negative Zone, finding and bonding with a symbiote there, as he had lost the original Carnage symbiote when Venom absorbed it into his own symbiote.\n",
      "\n",
      "History\n",
      "The earliest origins of the Negative Zone and its culture have not been revealed, but rough estimates place its height of science and art over 1.5 million years ago, nearly coinciding with the rise of the Skrull and Kree races. It is believed that around that time the Negative Zone ceased expanding and began its \"Big Crunch\", contracting toward a central nexus. Some of the most powerful and influential races thereby faced destruction and sought to preserve their lives.\n",
      "\n",
      "One of the more aggressive races at this time were the Tyannans. The lion-like bipeds explored much of the Negative Zone and eventually began to seed many of the planets, including Baluur, with their \"spores of life\". However, their final mission went awry.\n",
      "\n",
      "A debris field had begun forming around Tyanna, which was at the very heart of the Negative Zone. The Big Crunch had begun pulling the universe back towards its center and planets crumbled under the increased pressure. One of the last remaining Tyannan ships careened off a large chunk of rock and crashed on the desolate planet of Arthros. The ship's engines dead and food processors destroyed, the Tyannan captain ordered the release of their life spores. Life slowly began to evolve on the planet, much as it did on Earth.\n",
      "\n",
      "Farther from the core of the Negative Zone, other cultures thrived. One culture had gone so far as to cover half of their planet with a giant city, and then developed an artificial brain for it. The city, Ootah, even began to develop a sense of self-preservation and drove the inhabitants out of its boundaries, building up greater defenses to prevent them from returning.\n",
      "\n",
      "Farther still from the core, the world of Kestor also flourished. It was the first recorded world beyond Tyanna that faced obliteration at the hands of the Big Crunch, roughly 10,000 years ago. Unlike Tyanna, whose gravitational force became greater than could sustain life, Kestor's sun was pulled toward the nexus. This caused the planet's several moons to shift their respective orbits and wreak havoc on the planet itself. As 20,000 beings left Kestor in a great space ark, the sun exploded with the fury of a supernova, substantially damaging their ship. All but 500 of the crew died and the navigational systems were destroyed, eliminating all hope of finding a new planet to call home.\n",
      "\n",
      "One of the planets the Tyannans spored, Baluur, developed in a more barbarous manner. The inhabitants grew very large and powerful, and even the meek among them were stronger than an ordinary human. While they did progress technologically, their advances were at least partially driven by war. But they apparently began to experience the effects of the Big Crunch and several millennia ago they took their cities underneath the planet's surface. A sole entrance exists and is known only to the Baluurians themselves. They established a quiet monarchy and became very reclusive, only venturing into space on rare occasions.\n",
      "\n",
      "Sentient life eventually developed on Arthros. A lone insect-like creature emerged from the primordial marsh one thousand years ago, and began to reason. A frail creature, he used his superior intellect to avoid larger predators and soon stumbled upon the Tyannan's derelict ship. Inside the craft, he donned a helmet in an attempt to warm himself. The helmet in fact had recordings of all the Tyannan technology and culture, and the creature was deftly able to assimilate all of it. He took the power from discarded life canisters and created a Cosmic Control Rod, capable of granting the wielder great power. Taking the name Annihilus, he set out to not only right those who wronged him, but to ensure that no one should ever harm him again.\n",
      "\n",
      "While Annihilus forcibly took control of his immediate sector of space, a king rose to prominence on Baluur. Blastaar was a ruthless and powerful leader who sought to expand Baluurian domain. The other Baluurians feared him and were eventually able to depose him by sedating him heavily and sending him to what they hoped would be his destruction in the center of the Negative Zone.\n",
      "\n",
      "Discovery\n",
      "While searching for a way to travel through sub-space, Reed Richards stumbled upon a gateway to the Negative Zone. He spent a fair amount of time studying it through probes, and determined that it was largely unpopulated. So much so, in fact, that he—and others—used the Negative Zone on several occasions to get rid of difficult enemies, such as the Mad Thinker's android, the Super-Adaptoid, and even Galactus. Reed and the Fantastic Four have since done more detailed explorations of the Zone and no longer use it to dispose of villains.\n",
      "\n",
      "For some time, the Norse realm of Asgard was lost in the Negative Zone.\n",
      "\n",
      "Civil War\n",
      "\n",
      "In the Super-Hero Civil War, a group of heroes led by Iron Man, Mister Fantastic and Yellowjacket have created a massive prison in the Negative Zone (similar to the Vault) to house captured non-registering heroes as they wait for their trials. It is designated Negative Zone Prison Alpha but nicknamed Fantasy Island by inmates. Tony Stark himself named it \"Project 42\", as it had been the 42nd idea out of a hundred that he, Reed Richards and Hank Pym had created following the Stamford Disaster. There are portals to it planned for every single state so prisoners can be transported there by the different teams in the Fifty State Initiative- including one at Ryker's Island. It is very clean, with sanitation, but extremely heavily guarded, including password-changes every ten minutes. Its most notable former inmates were Iron Fist (posing as Daredevil at the time), Cloak and Dagger, Speedball, Prodigy and Prowler.\n",
      "\n",
      "It is also the setting for the final battle of the super hero civil war, as Iron Man lays a trap for Captain America but the Captain retaliates by using his planted mole, Hulkling, to release all the prisoners of 42 who come to his aid. Though in the ensuing battle Cloak manages to teleport all its superhuman detainees out, it remains as a prison for villains such as Lady Deathstrike and Taskmaster. (Taskmaster was later released to become the trainer for new Initiative recruits and Deathstrike seemingly escapes to assist in the Purifiers crusade during the Messiah Complex.) It also holds many of the Sakaaran forces from World War Hulk.\n",
      "\n",
      "In the one-shot Civil War: The Return, the Prison's warden was revealed as the first Captain Marvel, apparently back from the dead. Although he was later revealed as a Skrull sleeper agent named Khn'nr, whose conditioning was so strong he kept believing he was the Captain even after he had realized he really was not.\n",
      "\n",
      "Secret Invasion\n",
      "In the Secret Invasion: Fantastic Four limited series, the Skrull warrior Lyja (posing as Susan Richards) sends the Baxter Building into the Negative Zone. She reveals herself to Johnny Storm (her former spouse) and attacks him, feeling angry that he had forgotten her. During the course of their battle, Johnny saves Lyja from being hit by a police car, pulled in through the portal. The two reconcile after that, but a Negative Zone creature attacks them. They manage to defeat the creature, but Lyja passes out from her injuries. A bit later, when the \"new\" Fantastic Four fly off to the prison, Franklin and Valeria are grabbed by Negative Zone creatures, but Lyja saves them. Later when Ben, Johnny, Franklin, Val and the Tinkerer are ready to leave the Negative Zone she refuses to leave, because she wants to find out who she is.\n",
      "\n",
      "War of Kings\n",
      "After the events of Secret Invasion, the inmates of the prison took control of the facility after their correction officers abandoned the prison. Blastaar later overrun the prison during the prelude to the War of Kings story arc.\n",
      "\n",
      "Cataclysm\n",
      "The Negative Zone is used to dispose of Earth-616's Galactus when he is accidentally transferred to the Ultimate Marvel universe due to the temporal distortions caused by the events of Age of Ultron. The heroes of the Ultimate Marvel universe reasoned that Galactus will starve to death in the Negative Zone because it is a universe made of antimatter and Galactus would not have anything to eat there.\n",
      "\n",
      "Annihilation: Scourge\n",
      "The Negative Zone is later visited by the Sentry, who is struggling to understand his new state of self and find a way to separate himself from the Void. Upon soaking up the negative cosmic rays, the Sentry is able to separate himself from the Void but is left in the powerless form of Bob Reynolds. The Void was then drawn to the Cancerverse realm, where nothing could die, and impersonating and taking on the appearance of the Sentry, the Void became the new leader of its forces. He then led them back to the Negative Zone, forcing longtime enemies Blastaar and Annihilus to team up against them to prevent the Negative Zone kingdoms from being ravaged by the Undying Ones. Led by the Revengers, an ever-growing army of zombie-esque drones slaughter and infect their way from world to world giving no quarter as they leave destruction in their wake. As their desperate attempt to defend their domain fails, both rulers discover in the process that the Void is planning to conquer the Negative Zone and its denizens before returning to his own universe to spread the Cancerverse's infection even further. Annihilus then crosses into the positive matter universe to seek assistance, only to land right in the lap of Richard Ryder the last Nova, however, the past trauma at the hands of the Cancerverse, caused Ryder to flee, leaving Annihilus at the mercy of his enemies. The Void then attempt to cross over into the positive Universe, but was stopped by Beta Ray Bill and Lockjaw, with Beta Ray Bill forced to sacrifice his magical hammer, Stormbreaker, in the process.\n",
      "\n",
      "Unique features\n",
      "\n",
      "The Crossroads of Infinity\n",
      "Initially, it was generally believed that getting caught in the gravitational pull of the vortex at the center of the Zone meant certain death. However, some theorized that if one could survive entry into the vortex, one could travel to another dimension. However, this was only a theory until Doctor Doom gambled the lives of the Fantastic Four to prove it. Once inside the so-called Crossroads, individuals flip through a series of other parallel dimensions while progressing through. Coming to a halt seems to stabilize the Crossroads and then allows movement in the universe arrived in.\n",
      "\n",
      "However, it still remains a risky trip for a number of reasons. First and foremost is the expenditure of energy necessary to keep from imploding. On one occasion, it required the use of both Annihilus' Cosmic Control Rod and the Invisible Woman's force field to stave off death. Second, and almost equally important, is the lack of control one has while passing through different dimensions. Even with the precise calculations of Dr. Doom, it required several hops to reach the dimension he sought.\n",
      "\n",
      "Staying on a straight course to the very heart of the Crossroads leads one to Tyanna. Although it was believed to have been destroyed centuries earlier, the Tyannans' technology proved capable of sustaining them within the center of the vortex. Using their advanced machines, they were able to modify their bodies to exist safely in the immense pressures, but at the cost of being forced to remain hidden within the Crossroads. The Tyannans remain there, content to pursue their scientific interests.\n",
      "\n",
      "The Distortion Area\n",
      "Normally beings enter the Negative Zone through the Distortion Area. This is an invisible sphere of energy that resides in the Negative Zone but is accessible from many parts of Earth. By hitting the field with a precise wavelength of energy, a rift opens between both dimensions connected by the Distortion Area. This area acts as a buffer between the two polar opposite universes and alters a traveler's own polarity so that they may exist in the other dimension without harm.\n",
      "\n",
      "When activated, the Distortion Area appears from the outside as a crackling energy source roughly six feet in circumference. This effect only lasts as long as the field is activated and, once closed, becomes invisible again. Nearby matter is sucked into the near-vacuum of the Distortion Area and \"falls\" for about 50 seconds before emerging on the other side.\n",
      "\n",
      "The Distortion Area itself is nothing short of indescribable. Humans cannot begin to accurately fathom or record what transpires within the Distortion Area and travelers' minds try to compensate with a bizarre display of light and color. Combined with the natural turbulence in the area, many find the trip rather nauseating.\n",
      "\n",
      "Like any other mode of transport, choosing where to enter the Distortion Area in part affects where an individual is deposited on the other side. Reed Richards' former lab in the Baxter Building, for example, deposited someone at the outskirts of the Debris Field near Arthros. A rift opened on Yancy Street, however, dropped a traveler on the planet Tarsuu.\n",
      "\n",
      "Other methods may be employed to reach the Negative Zone but, while they tend to be more spatially accurate, they are very difficult to come by. Generally, this mode of transport is reserved by extremely powerful entities like Thor, the Supreme Intelligence, Galactus and the Watchers. Captain Mar-Vell's Nega-Bands could also transport the wearer to the Negative Zone but the precise destination was dependent upon the location of the Nega-Band's counterparts worn by Rick Jones.\n",
      "\n",
      "Emotions\n",
      "One of the rather undocumented side-effects the Negative Zone has on people seems to involve bringing forth what are usually considered negative emotions. At this point, it is difficult to determine the cause of this unusual phenomenon, but a number of dealings with the Negative Zone have given the impression that something inherent within it can lead to emotional distress.\n",
      "\n",
      "The severity of this distress can vary greatly. On a few occasions, it has manifested itself as a citywide panic. Other times, it merely caused a single person to temporarily lose their hope. Still other times, it caused several individuals to experience emotionally charged flashbacks, pulled from their subconscious. And, although it could be attributed to his psyche, Annihilus' main motivation is an obsessive fear of death.\n",
      "\n",
      "Not all inhabitants of the Negative Zone experience anything beyond normal anxieties, but the number of known instances of unusual psychological distress seems rather high. Visitors to the Zone may or may not experience anything emotionally disturbing. While there is no concrete evidence to show the correlation the Negative Zone has with emotional discord, it remains a significant feature of the region.\n",
      "\n",
      "Life\n",
      "As mentioned previously, the Negative Zone was originally thought to be largely uninhabited. Though this is now known to be incorrect, life having arisen on a number of its planets, the Zone is still mostly unoccupied space. Most of the encounters heroes have had with the Negative Zone revolve around Annihilus and/or Blastaar.\n",
      "\n",
      "Curiously, for as little life as there exists in the Negative Zone, it is very capable of supporting life. There, outer space itself is permeated with an oxygen-rich atmosphere, closely approximating Earth's. Consequently, humans can exist peacefully in space without the need for cumbersome pressure suits or oxygen masks.\n",
      "\n",
      "It is perhaps an issue of gravitational pull that is one of the biggest hindrances to life in the Negative Zone. While all objects of reasonably sized mass (planets, moons, asteroids, etc.) obviously have their own gravitational pull, it is weak enough to be overcome with minimal effort. Most heroes with flight capabilities can escape a planet's gravitational field with ease, as can any machine with the capacity for flight. Because of this lowered gravity, it is believed that vegetation has difficulty seeding properly, giving life a tenuous foothold at best on any given planet.\n",
      "\n",
      "Moisture also seems to be an issue in the dearth of life in the Negative Zone. While each planet has hardly been explored in full, many of the ones studied—including those that are inhabited—have shown few, if any, natural water sources. Most planets appear to be very arid, with cultures adapting to the lack of moisture much as was done on Earth's deserts.\n",
      "\n",
      "Time\n",
      "Being a different dimension with different laws of physics, time flows differently in the Negative Zone than it does on Earth. Although a comprehensive analysis has never been completed, preliminary findings suggest that two weeks pass in the Negative Zone for every hour on Earth, making a 336:1 ratio. On a smaller scale, every minute on Earth is a little over five and half hours in the Negative Zone.\n",
      "\n",
      "It seems, however, that this ratio changes as one approaches the center of the Negative Zone. Since there have been several instances of pan-dimensional conversations between the two planes, it appears the time ratio is much closer to 1:1 when one is at the nexus of the Negative Zone. It is currently unclear at what rate the time difference increases as one moves away from the vortex or if there is an upper limit to how great the difference between the two universes can become.\n",
      "\n",
      "Planets\n",
      "The Negative Zone includes the following planets:\n",
      "\n",
      " Argor – A planet that is home to the Argorans.\n",
      " Arthros – A planet that is home to Annihilus and the Arthosians (a race of insectoids).\n",
      " Baluur – A planet that is home to Blastaar.\n",
      " Kestor – A planet that was destroyed causing the Kestorians to become nomads.\n",
      " Tarsuu – \n",
      " Tyanna – A planet that is home to the Tyannans (a race of lion-like aliens). Tyanna is located at the Crossroads of Infinity. The Tyannan scientists genetically engineered a spore that could be scattered over an uninhabited planet's surface where they can grow into new plants and animals. This terraforming made the planet habitable.\n",
      "\n",
      "Other versions\n",
      "In the Blink miniseries, it is established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Blink from the Age of Apocalypse found herself in a Negative Zone that still remembered the Fantastic Four's role in defeating Annihilus despite the fact that the team never existed in her world. This goes along with the Mutant X Universe, where Havok tried to get back from the Mutant X-verse to the main Marvel earth by a detour through the Negative Zone. Additionally both Rikki Barnes and Onslaught traveled from the Pocket Universe of Heroes Reborn to the mainstream Marvel Universe through the Negative Zone. During the Cataclysm event it is again established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Ultimate Marvel heroes, after a brief trip to Earth-616 to acquire local information on Galactus, eventually manage to send Galactus to the Negative Zone, reasoning that he will eventually starve to death because Negative Zone is made of anti-matter, however, following an incident on Earth 616 where the Eternal known as Ikaris is brainwashed by a Kree device called the God's Whisper, the Eternals retrieve, with help from Aarkus, the comatose Galactus from the Negative Zone, and state that they plan to use the God's Whisper to unleash him upon the Kree when he awakens as revenge for what they did to Ikaris.\n",
      "\n",
      "Ultimate Marvel (The N-Zone)\n",
      "The Ultimate Marvel equivalent of the Negative Zone was originally called the N-Zone, a zone that exists directly below the Ultimate Marvel universe. It is tied to the powers granted to the Fantastic Four and is the homeplace of the Ultimate Fantastic Four villain Nihil. The N-Zone is a universe in the later stages of entropic heat death, with less than a million years of existence left to it. There are few stars still burning, mostly long-lived red dwarf stars, and despite the advanced technology of many races, life barely maintains a toehold on its existence. Space is also apparently foreshortened in this universe, with the Fantastic Four's shuttle Awesome achieving speeds that would be impossible in our own universe. It has an atmosphere that is lethally acidic to humans and the Ultimate Fantastic Four, with the exception of the Thing, required space suits to live.\n",
      "\n",
      "Later it was reckoned that the N-Zone is one of many other zones, labeled with letters (e.g. the Z-Zone and the Q-Zone) implying that the N is merely a categorization, not a shortening of the word \"negative\" therefore no logger connected to the Negative Zone. A proper Negative Zone was eventually introduced in the Ultimate Marvel where The Ultimates confront Reed Richards.\n",
      "\n",
      "Heroes Reborn (2021)\n",
      "In an alternate reality depicted in the 2021 \"Heroes Reborn\" miniseries, the Negative Zone is used by the Squadron Supreme of America to imprison Earth's most dangerous villains, such as General Annihilus, Doctor Juggernaut, the Hulk, Mister Beyonder, Namor, and Hank Pym / Ultron.\n",
      "\n",
      "In other media\n",
      "\n",
      "Television\n",
      " The Negative Zone appears in the 1994 Fantastic Four animated series episode, \"Behold the Negative Zone\".\n",
      " In a scripted, but unproduced episode of The Silver Surfer titled \"Down to Earth\" Part 3, Reed Richards uses the Negative Zone as a means to contain Terrax.\n",
      " The Negative Zone appears in Fantastic Four: World's Greatest Heroes. This version is depicted as an alternate dimension inhabited by an array of various serpentine and insectoid creatures. According to Doctor Doom, the Negative Zone is a nexus for various other universes and claims that he discovered it years before Reed Richards.\n",
      " The Negative Zone appears in the two-part Iron Man: Armored Adventures episode \"The Makluan Invasion\". It is depicted as a dark empty void which the Mandarin banished S.H.I.E.L.D.'s Helicarrier to before rescuing it at the end of the episode following the titular event.\n",
      " The Negative Zone appears in The Avengers: Earth's Mightiest Heroes. It debuts in the episode \"The Man Who Stole Tomorrow\", when Thor, Ant-Man, and the Wasp place Blizzard in Negative Zone Prison 42. In the episode \"Assault on 42\", Annihilus and his insectoid slaves attack Prison 42, but were defeated by the joint forces of Captain America, Thor, Wasp, Miss Marvel, and the imprisoned supervillains of Prison 42. In the episode \"Avengers Assemble\", Mister Fantastic and Iron Man create a portal to the Negative Zone to send Galactus there to feed on its infinite anti-matter instead of Earth, using Galactus's own emitted energy as a tractor beam.\n",
      " The Negative Zone appears in the Hulk and the Agents of S.M.A.S.H. episodes \"Doorway to Destruction\", \"Into the Negative Zone\", and \"Days of Future Smash\" Pt. 5.\n",
      "\n",
      "Film\n",
      "In the original script for the Fantastic Four reboot by Jeremy Slater, the Negative Zone was reinvented as a planet that was home to ancient alien civilization before being destroyed by Galactus. However, after re-shoots and edits, the Negative Zone was renamed Planet Zero, and reworked into a planet with a cracked landscape and unstable energy that becomes the source of the titular characters' and Victor von Doom's mutations.\n",
      "\n",
      "Video games\n",
      " The Negative Zone appears in Marvel: Ultimate Alliance 2. It contains \"Prison 42\", a prison for unregistered heroes and nanite-controlled supervillains, and where the Fold first becomes active before Nick Fury destroyed it to keep the Fold from invading Earth.\n",
      " The Negative Zone appears in the Marvel Super Hero Squad: The Infinity Gauntlet video game.\n",
      "\n",
      "See also\n",
      " Phantom Zone\n",
      " Qward\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " Negative Zone at Marvel.com\n",
      " Negative Zone at Marvel Wiki\n",
      "\n",
      "Marvel Comics dimensions\n"
     ]
    }
   ],
   "source": [
    "from dataloader import load_data\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "corpus, _, _ = load_data(\"mldr\")\n",
    "key = random.choice(list(corpus.keys()))\n",
    "doc_mldr = corpus[key]['text']\n",
    "for line in doc_mldr.split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 In other media\n",
      "Television\n",
      "Film\n",
      "\n",
      "doc0|2 Video games\n",
      "\n",
      "doc0|3 See also\n",
      "References\n",
      "External links\n",
      "\n",
      "doc0|4 The Negative Zone is a fictional setting, an antimatter universe appearing in American comic books published by Marvel Comics.\n",
      "The location is depicted in various publications from Marvel, most frequently in Fantastic Four and Captain Marvel.\n",
      "Created by Stan Lee and Jack Kirby, it first appeared in Fantastic Four #51 (June 1966).\n",
      "The Negative Zone in the Marvel Universe is used as a fictional dimension.\n",
      "Essentially, it is a universe parallel to Earth's.\n",
      "The two have many similarities, but a few noteworthy differences include: all matter in the Negative Zone is negatively charged; the Negative Zone is entirely filled with a pressurized, breathable atmosphere; and near the center of the Negative Zone is a deadly vortex of unspeakable power.\n",
      "Since the Negative Zone is largely uninhabited, several would-be conquerors have attempted to bridge the gap to Earth and take over its population.\n",
      "A few notable residents of the Negative Zone include Blastaar and Annihilus.\n",
      "The Negative Zone is often visited by the Fantastic Four as Mister Fantastic discovered it, and has mapped portions of it extensively.\n",
      "The Age of Apocalypse version of Blink also visited it once, which proved that there's only one Negative Zone, as Annihilus remembered the Fantastic Four even though they never existed in the Age of Apocalypse.\n",
      "For a number of years, Captain Mar-Vell and Rick Jones were bonded to each other, causing one of them to exist in the Negative Zone while the other would exist in the regular universe.\n",
      "Spider-Man has also visited the Negative Zone, and acquired a costume that allowed him to merge with shadows and become practically invisible.\n",
      "When he was framed by Norman Osborn a few weeks later, he used the costume to become the dark, mysterious Dusk (one of his four new superhero identities during the Identity Crisis story arc).\n",
      "A few months later after Spider-Man's name was cleared, Cassie St. Commons was given the guise of Dusk and joined the Slingers.\n",
      "Cletus Kasady also visited the Negative Zone, finding and bonding with a symbiote there, as he had lost the original Carnage symbiote when Venom absorbed it into his own symbiote.\n",
      "History\n",
      "The earliest origins of the Negative Zone and its culture have not been revealed, but rough estimates place its height of science and art over 1.5 million years ago, nearly coinciding with the rise of the Skrull and Kree races.\n",
      "It is believed that around that time the Negative Zone ceased expanding and began its \"Big Crunch\", contracting toward a central nexus.\n",
      "Some of the most powerful and influential races thereby faced destruction and sought to preserve their lives.\n",
      "One of the more aggressive races at this time were the Tyannans.\n",
      "The lion-like bipeds explored much of the Negative Zone and eventually began to seed many of the planets, including Baluur, with their \"spores of life\".\n",
      "However, their final mission went awry.\n",
      "A debris field had begun forming around Tyanna, which was at the very heart of the Negative Zone.\n",
      "The Big Crunch had begun pulling the universe back towards its center and planets crumbled under the increased pressure.\n",
      "One of the last remaining Tyannan ships careened off a large chunk of rock and crashed on the desolate planet of Arthros.\n",
      "The ship's engines dead and food processors destroyed, the Tyannan captain ordered the release of their life spores.\n",
      "Life slowly began to evolve on the planet, much as it did on Earth.\n",
      "Farther from the core of the Negative Zone, other cultures thrived.\n",
      "One culture had gone so far as to cover half of their planet with a giant city, and then developed an artificial brain for it.\n",
      "The city, Ootah, even began to develop a sense of self-preservation and drove the inhabitants out of its boundaries, building up greater defenses to prevent them from returning.\n",
      "Farther still from the core, the world of Kestor also flourished.\n",
      "It was the first recorded world beyond Tyanna that faced obliteration at the hands of the Big Crunch, roughly 10,000 years ago.\n",
      "Unlike Tyanna, whose gravitational force became greater than could sustain life, Kestor's sun was pulled toward the nexus.\n",
      "This caused the planet's several moons to shift their respective orbits and wreak havoc on the planet itself.\n",
      "As 20,000 beings left Kestor in a great space ark, the sun exploded with the fury of a supernova, substantially damaging their ship.\n",
      "All but 500 of the crew died and the navigational systems were destroyed, eliminating all hope of finding a new planet to call home.\n",
      "One of the planets the Tyannans spored, Baluur, developed in a more barbarous manner.\n",
      "The inhabitants grew very large and powerful, and even the meek among them were stronger than an ordinary human.\n",
      "While they did progress technologically, their advances were at least partially driven by war.\n",
      "But they apparently began to experience the effects of the Big Crunch and several millennia ago they took their cities underneath the planet's surface.\n",
      "A sole entrance exists and is known only to the Baluurians themselves.\n",
      "They established a quiet monarchy and became very reclusive, only venturing into space on rare occasions.\n",
      "Sentient life eventually developed on Arthros.\n",
      "A lone insect-like creature emerged from the primordial marsh one thousand years ago, and began to reason.\n",
      "A frail creature, he used his superior intellect to avoid larger predators and soon stumbled upon the Tyannan's derelict ship.\n",
      "Inside the craft, he donned a helmet in an attempt to warm himself.\n",
      "The helmet in fact had recordings of all the Tyannan technology and culture, and the creature was deftly able to assimilate all of it.\n",
      "He took the power from discarded life canisters and created a Cosmic Control Rod, capable of granting the wielder great power.\n",
      "Taking the name Annihilus, he set out to not only right those who wronged him, but to ensure that no one should ever harm him again.\n",
      "While Annihilus forcibly took control of his immediate sector of space, a king rose to prominence on Baluur.\n",
      "Blastaar was a ruthless and powerful leader who sought to expand Baluurian domain.\n",
      "The other Baluurians feared him and were eventually able to depose him by sedating him heavily and sending him to what they hoped would be his destruction in the center of the Negative Zone.\n",
      "While searching for a way to travel through sub-space, Reed Richards stumbled upon a gateway to the Negative Zone.\n",
      "He spent a fair amount of time studying it through probes, and determined that it was largely unpopulated.\n",
      "So much so, in fact, that he—and others—used the Negative Zone on several occasions to get rid of difficult enemies, such as the Mad Thinker's android, the Super-Adaptoid, and even Galactus.\n",
      "Reed and the Fantastic Four have since done more detailed explorations of the Zone and no longer use it to dispose of villains.\n",
      "For some time, the Norse realm of Asgard was lost in the Negative Zone.\n",
      "Civil War\n",
      "In the Super-Hero Civil War, a group of heroes led by Iron Man, Mister Fantastic and Yellowjacket have created a massive prison in the Negative Zone (similar to the Vault) to house captured non-registering heroes as they wait for their trials.\n",
      "It is designated Negative Zone Prison Alpha but nicknamed Fantasy Island by inmates.\n",
      "Tony Stark himself named it \"Project 42\", as it had been the 42nd idea out of a hundred that he, Reed Richards and Hank Pym had created following the Stamford Disaster.\n",
      "There are portals to it planned for every single state so prisoners can be transported there by the different teams in the Fifty State Initiative- including one at Ryker's Island.\n",
      "It is very clean, with sanitation, but extremely heavily guarded, including password-changes every ten minutes.\n",
      "Its most notable former inmates were Iron Fist (posing as Daredevil at the time), Cloak and Dagger, Speedball, Prodigy and Prowler.\n",
      "It is also the setting for the final battle of the super hero civil war, as Iron Man lays a trap for Captain America but the Captain retaliates by using his planted mole, Hulkling, to release all the prisoners of 42 who come to his aid.\n",
      "Though in the ensuing battle Cloak manages to teleport all its superhuman detainees out, it remains as a prison for villains such as Lady Deathstrike and Taskmaster.\n",
      "(Taskmaster was later released to become the trainer for new Initiative recruits and Deathstrike seemingly escapes to assist in the Purifiers crusade during the Messiah Complex.)\n",
      "It also holds many of the Sakaaran forces from World War Hulk.\n",
      "In the one-shot Civil War: The Return, the Prison's warden was revealed as the first Captain Marvel, apparently back from the dead.\n",
      "Although he was later revealed as a Skrull sleeper agent named Khn'nr, whose conditioning was so strong he kept believing he was the Captain even after he had realized he really was not.\n",
      "Secret Invasion\n",
      "In the Secret Invasion: Fantastic Four limited series, the Skrull warrior Lyja (posing as Susan Richards) sends the Baxter Building into the Negative Zone.\n",
      "She reveals herself to Johnny Storm (her former spouse) and attacks him, feeling angry that he had forgotten her.\n",
      "During the course of their battle, Johnny saves Lyja from being hit by a police car, pulled in through the portal.\n",
      "The two reconcile after that, but a Negative Zone creature attacks them.\n",
      "They manage to defeat the creature, but Lyja passes out from her injuries.\n",
      "A bit later, when the \"new\" Fantastic Four fly off to the prison, Franklin and Valeria are grabbed by Negative Zone creatures, but Lyja saves them.\n",
      "Later when Ben, Johnny, Franklin, Val and the Tinkerer are ready to leave the Negative Zone she refuses to leave, because she wants to find out who she is.\n",
      "War of Kings\n",
      "After the events of Secret Invasion, the inmates of the prison took control of the facility after their correction officers abandoned the prison.\n",
      "Blastaar later overrun the prison during the prelude to the War of Kings story arc.\n",
      "Cataclysm\n",
      "The Negative Zone is used to dispose of Earth-616's Galactus when he is accidentally transferred to the Ultimate Marvel universe due to the temporal distortions caused by the events of Age of Ultron.\n",
      "The heroes of the Ultimate Marvel universe reasoned that Galactus will starve to death in the Negative Zone because it is a universe made of antimatter and Galactus would not have anything to eat there.\n",
      "Annihilation: Scourge\n",
      "The Negative Zone is later visited by the Sentry, who is struggling to understand his new state of self and find a way to separate himself from the Void.\n",
      "Upon soaking up the negative cosmic rays, the Sentry is able to separate himself from the Void but is left in the powerless form of Bob Reynolds.\n",
      "The Void was then drawn to the Cancerverse realm, where nothing could die, and impersonating and taking on the appearance of the Sentry, the Void became the new leader of its forces.\n",
      "He then led them back to the Negative Zone, forcing longtime enemies Blastaar and Annihilus to team up against them to prevent the Negative Zone kingdoms from being ravaged by the Undying Ones.\n",
      "Led by the Revengers, an ever-growing army of zombie-esque drones slaughter and infect their way from world to world giving no quarter as they leave destruction in their wake.\n",
      "As their desperate attempt to defend their domain fails, both rulers discover in the process that the Void is planning to conquer the Negative Zone and its denizens before returning to his own universe to spread the Cancerverse's infection even further.\n",
      "Annihilus then crosses into the positive matter universe to seek assistance, only to land right in the lap of Richard Ryder the last Nova, however, the past trauma at the hands of the Cancerverse, caused Ryder to flee, leaving Annihilus at the mercy of his enemies.\n",
      "The Void then attempt to cross over into the positive Universe, but was stopped by Beta Ray Bill and Lockjaw, with Beta Ray Bill forced to sacrifice his magical hammer, Stormbreaker, in the process.\n",
      "The Crossroads of Infinity\n",
      "Initially, it was generally believed that getting caught in the gravitational pull of the vortex at the center of the Zone meant certain death.\n",
      "However, some theorized that if one could survive entry into the vortex, one could travel to another dimension.\n",
      "However, this was only a theory until Doctor Doom gambled the lives of the Fantastic Four to prove it.\n",
      "Once inside the so-called Crossroads, individuals flip through a series of other parallel dimensions while progressing through.\n",
      "Coming to a halt seems to stabilize the Crossroads and then allows movement in the universe arrived in.\n",
      "However, it still remains a risky trip for a number of reasons.\n",
      "First and foremost is the expenditure of energy necessary to keep from imploding.\n",
      "On one occasion, it required the use of both Annihilus' Cosmic Control Rod and the Invisible Woman's force field to stave off death.\n",
      "Second, and almost equally important, is the lack of control one has while passing through different dimensions.\n",
      "Even with the precise calculations of Dr. Doom, it required several hops to reach the dimension he sought.\n",
      "Staying on a straight course to the very heart of the Crossroads leads one to Tyanna.\n",
      "Although it was believed to have been destroyed centuries earlier, the Tyannans' technology proved capable of sustaining them within the center of the vortex.\n",
      "Using their advanced machines, they were able to modify their bodies to exist safely in the immense pressures, but at the cost of being forced to remain hidden within the Crossroads.\n",
      "The Tyannans remain there, content to pursue their scientific interests.\n",
      "The Distortion Area\n",
      "Normally beings enter the Negative Zone through the Distortion Area.\n",
      "This is an invisible sphere of energy that resides in the Negative Zone but is accessible from many parts of Earth.\n",
      "By hitting the field with a precise wavelength of energy, a rift opens between both dimensions connected by the Distortion Area.\n",
      "This area acts as a buffer between the two polar opposite universes and alters a traveler's own polarity so that they may exist in the other dimension without harm.\n",
      "When activated, the Distortion Area appears from the outside as a crackling energy source roughly six feet in circumference.\n",
      "Nearby matter is sucked into the near-vacuum of the Distortion Area and \"falls\" for about 50 seconds before emerging on the other side.\n",
      "The Distortion Area itself is nothing short of indescribable.\n",
      "Humans cannot begin to accurately fathom or record what transpires within the Distortion Area and travelers' minds try to compensate with a bizarre display of light and color.\n",
      "Combined with the natural turbulence in the area, many find the trip rather nauseating.\n",
      "Like any other mode of transport, choosing where to enter the Distortion Area in part affects where an individual is deposited on the other side.\n",
      "Reed Richards' former lab in the Baxter Building, for example, deposited someone at the outskirts of the Debris Field near Arthros.\n",
      "A rift opened on Yancy Street, however, dropped a traveler on the planet Tarsuu.\n",
      "Other methods may be employed to reach the Negative Zone but, while they tend to be more spatially accurate, they are very difficult to come by.\n",
      "Generally, this mode of transport is reserved by extremely powerful entities like Thor, the Supreme Intelligence, Galactus and the Watchers.\n",
      "Captain Mar-Vell's Nega-Bands could also transport the wearer to the Negative Zone but the precise destination was dependent upon the location of the Nega-Band's counterparts worn by Rick Jones.\n",
      "Emotions\n",
      "One of the rather undocumented side-effects the Negative Zone has on people seems to involve bringing forth what are usually considered negative emotions.\n",
      "At this point, it is difficult to determine the cause of this unusual phenomenon, but a number of dealings with the Negative Zone have given the impression that something inherent within it can lead to emotional distress.\n",
      "The severity of this distress can vary greatly.\n",
      "On a few occasions, it has manifested itself as a citywide panic.\n",
      "Other times, it merely caused a single person to temporarily lose their hope.\n",
      "Still other times, it caused several individuals to experience emotionally charged flashbacks, pulled from their subconscious.\n",
      "And, although it could be attributed to his psyche, Annihilus' main motivation is an obsessive fear of death.\n",
      "Not all inhabitants of the Negative Zone experience anything beyond normal anxieties, but the number of known instances of unusual psychological distress seems rather high.\n",
      "Visitors to the Zone may or may not experience anything emotionally disturbing.\n",
      "While there is no concrete evidence to show the correlation the Negative Zone has with emotional discord, it remains a significant feature of the region.\n",
      "Life\n",
      "As mentioned previously, the Negative Zone was originally thought to be largely uninhabited.\n",
      "Though this is now known to be incorrect, life having arisen on a number of its planets, the Zone is still mostly unoccupied space.\n",
      "Most of the encounters heroes have had with the Negative Zone revolve around Annihilus and/or Blastaar.\n",
      "Curiously, for as little life as there exists in the Negative Zone, it is very capable of supporting life.\n",
      "There, outer space itself is permeated with an oxygen-rich atmosphere, closely approximating Earth's.\n",
      "Consequently, humans can exist peacefully in space without the need for cumbersome pressure suits or oxygen masks.\n",
      "It is perhaps an issue of gravitational pull that is one of the biggest hindrances to life in the Negative Zone.\n",
      "While all objects of reasonably sized mass (planets, moons, asteroids, etc.) obviously have their own gravitational pull, it is weak enough to be overcome with minimal effort.\n",
      "Most heroes with flight capabilities can escape a planet's gravitational field with ease, as can any machine with the capacity for flight.\n",
      "Because of this lowered gravity, it is believed that vegetation has difficulty seeding properly, giving life a tenuous foothold at best on any given planet.\n",
      "Moisture also seems to be an issue in the dearth of life in the Negative Zone.\n",
      "While each planet has hardly been explored in full, many of the ones studied—including those that are inhabited—have shown few, if any, natural water sources.\n",
      "Most planets appear to be very arid, with cultures adapting to the lack of moisture much as was done on Earth's deserts.\n",
      "Time\n",
      "Being a different dimension with different laws of physics, time flows differently in the Negative Zone than it does on Earth.\n",
      "Although a comprehensive analysis has never been completed, preliminary findings suggest that two weeks pass in the Negative Zone for every hour on Earth, making a 336:1 ratio.\n",
      "On a smaller scale, every minute on Earth is a little over five and half hours in the Negative Zone.\n",
      "It seems, however, that this ratio changes as one approaches the center of the Negative Zone.\n",
      "Since there have been several instances of pan-dimensional conversations between the two planes, it appears the time ratio is much closer to 1:1 when one is at the nexus of the Negative Zone.\n",
      "It is currently unclear at what rate the time difference increases as one moves away from the vortex or if there is an upper limit to how great the difference between the two universes can become.\n",
      "Planets\n",
      "The Negative Zone includes the following planets:\n",
      "Argor – A planet that is home to the Argorans.\n",
      "Arthros – A planet that is home to Annihilus and the Arthosians (a race of insectoids).\n",
      "Baluur – A planet that is home to Blastaar.\n",
      "Kestor – A planet that was destroyed causing the Kestorians to become nomads.\n",
      "Tarsuu –\n",
      "Tyanna – A planet that is home to the Tyannans (a race of lion-like aliens).\n",
      "Tyanna is located at the Crossroads of Infinity.\n",
      "The Tyannan scientists genetically engineered a spore that could be scattered over an uninhabited planet's surface where they can grow into new plants and animals.\n",
      "This terraforming made the planet habitable.\n",
      "Other versions\n",
      "In the Blink miniseries, it is established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Blink from the Age of Apocalypse found herself in a Negative Zone that still remembered the Fantastic Four's role in defeating Annihilus despite the fact that the team never existed in her world.\n",
      "This goes along with the Mutant X Universe, where Havok tried to get back from the Mutant X-verse to the main Marvel earth by a detour through the Negative Zone.\n",
      "Additionally both Rikki Barnes and Onslaught traveled from the Pocket Universe of Heroes Reborn to the mainstream Marvel Universe through the Negative Zone.\n",
      "During the Cataclysm event it is again established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Ultimate Marvel heroes, after a brief trip to Earth-616 to acquire local information on Galactus, eventually manage to send Galactus to the Negative Zone, reasoning that he will eventually starve to death because Negative Zone is made of anti-matter, however, following an incident on Earth 616 where the Eternal known as Ikaris is brainwashed by a Kree device called the God's Whisper, the Eternals retrieve, with help from Aarkus, the comatose Galactus from the Negative Zone, and state that they plan to use the God's Whisper to unleash him upon the Kree when he awakens as revenge for what they did to Ikaris.\n",
      "Ultimate Marvel (The N-Zone)\n",
      "The Ultimate Marvel equivalent of the Negative Zone was originally called the N-Zone, a zone that exists directly below the Ultimate Marvel universe.\n",
      "It is tied to the powers granted to the Fantastic Four and is the homeplace of the Ultimate Fantastic Four villain Nihil.\n",
      "The N-Zone is a universe in the later stages of entropic heat death, with less than a million years of existence left to it.\n",
      "There are few stars still burning, mostly long-lived red dwarf stars, and despite the advanced technology of many races, life barely maintains a toehold on its existence.\n",
      "Space is also apparently foreshortened in this universe, with the Fantastic Four's shuttle Awesome achieving speeds that would be impossible in our own universe.\n",
      "It has an atmosphere that is lethally acidic to humans and the Ultimate Fantastic Four, with the exception of the Thing, required space suits to live.\n",
      "Later it was reckoned that the N-Zone is one of many other zones, labeled with letters (e.g. the Z-Zone and the Q-Zone) implying that the N is merely a categorization, not a shortening of the word \"negative\" therefore no logger connected to the Negative Zone.\n",
      "A proper Negative Zone was eventually introduced in the Ultimate Marvel where The Ultimates confront Reed Richards.\n",
      "Heroes Reborn (2021)\n",
      "In an alternate reality depicted in the 2021 \"Heroes Reborn\" miniseries, the Negative Zone is used by the Squadron Supreme of America to imprison Earth's most dangerous villains, such as General Annihilus, Doctor Juggernaut, the Hulk, Mister Beyonder, Namor, and Hank Pym / Ultron.\n",
      "The Negative Zone appears in the 1994 Fantastic Four animated series episode, \"Behold the Negative Zone\".\n",
      "In a scripted, but unproduced episode of The Silver Surfer titled \"Down to Earth\" Part 3, Reed Richards uses the Negative Zone as a means to contain Terrax.\n",
      "The Negative Zone appears in Fantastic Four: World's Greatest Heroes.\n",
      "This version is depicted as an alternate dimension inhabited by an array of various serpentine and insectoid creatures.\n",
      "According to Doctor Doom, the Negative Zone is a nexus for various other universes and claims that he discovered it years before Reed Richards.\n",
      "The Negative Zone appears in the two-part Iron Man: Armored Adventures episode \"The Makluan Invasion\".\n",
      "It is depicted as a dark empty void which the Mandarin banished S.H.I.E.L.D.'s Helicarrier to before rescuing it at the end of the episode following the titular event.\n",
      "The Negative Zone appears in The Avengers: Earth's Mightiest Heroes.\n",
      "It debuts in the episode \"The Man Who Stole Tomorrow\", when Thor, Ant-Man, and the Wasp place Blizzard in Negative Zone Prison 42.\n",
      "In the episode \"Assault on 42\", Annihilus and his insectoid slaves attack Prison 42, but were defeated by the joint forces of Captain America, Thor, Wasp, Miss Marvel, and the imprisoned supervillains of Prison 42.\n",
      "In the episode \"Avengers Assemble\", Mister Fantastic and Iron Man create a portal to the Negative Zone to send Galactus there to feed on its infinite anti-matter instead of Earth, using Galactus's own emitted energy as a tractor beam.\n",
      "The Negative Zone appears in the Hulk and the Agents of S.M.A.S.H. episodes \"Doorway to Destruction\", \"Into the Negative Zone\", and \"Days of Future Smash\" Pt. 5.\n",
      "In the original script for the Fantastic Four reboot by Jeremy Slater, the Negative Zone was reinvented as a planet that was home to ancient alien civilization before being destroyed by Galactus.\n",
      "However, after re-shoots and edits, the Negative Zone was renamed Planet Zero, and reworked into a planet with a cracked landscape and unstable energy that becomes the source of the titular characters' and Victor von Doom's mutations.\n",
      "The Negative Zone appears in Marvel: Ultimate Alliance 2.\n",
      "It contains \"Prison 42\", a prison for unregistered heroes and nanite-controlled supervillains, and where the Fold first becomes active before Nick Fury destroyed it to keep the Fold from invading Earth.\n",
      "The Negative Zone appears in the Marvel Super Hero Squad: The Infinity Gauntlet video game.\n",
      "Phantom Zone\n",
      "Negative Zone at Marvel.com\n",
      "Negative Zone at Marvel Wiki\n",
      "Marvel Comics dimensions\n",
      "\n",
      "doc0|5 They exchanged places by clasping the special bracelets each wore or automatically after a few hours.\n",
      "\n",
      "doc0|6 This effect only lasts as long as the field is activated and, once closed, becomes invisible again.\n",
      "\n",
      "doc0|7 Discovery\n",
      "\n",
      "doc0|8 Fictional description\n",
      "\n",
      "doc0|9 Qward\n",
      "\n",
      "doc0|10 Unique features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_split(doc_mldr, doc_id=\"doc0\", n_clusters=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Fictional description\n",
      "They exchanged places by clasping the special bracelets each wore or automatically after a few hours.\n",
      "History\n",
      "Discovery\n",
      "It is very clean, with sanitation, but extremely heavily guarded, including password-changes every ten minutes.\n",
      "Unique features\n",
      "This effect only lasts as long as the field is activated and, once closed, becomes invisible again.\n",
      "Life\n",
      "Time\n",
      "Tarsuu –\n",
      "Other versions\n",
      "In other media\n",
      "Television\n",
      "Film\n",
      "Video games\n",
      "See also\n",
      "Qward\n",
      "References\n",
      "External links\n",
      "\n",
      "doc0|2 The Negative Zone is a fictional setting, an antimatter universe appearing in American comic books published by Marvel Comics.\n",
      "The location is depicted in various publications from Marvel, most frequently in Fantastic Four and Captain Marvel.\n",
      "Created by Stan Lee and Jack Kirby, it first appeared in Fantastic Four #51 (June 1966).\n",
      "The Negative Zone in the Marvel Universe is used as a fictional dimension.\n",
      "Essentially, it is a universe parallel to Earth's.\n",
      "The two have many similarities, but a few noteworthy differences include: all matter in the Negative Zone is negatively charged; the Negative Zone is entirely filled with a pressurized, breathable atmosphere; and near the center of the Negative Zone is a deadly vortex of unspeakable power.\n",
      "Since the Negative Zone is largely uninhabited, several would-be conquerors have attempted to bridge the gap to Earth and take over its population.\n",
      "A few notable residents of the Negative Zone include Blastaar and Annihilus.\n",
      "The Negative Zone is often visited by the Fantastic Four as Mister Fantastic discovered it, and has mapped portions of it extensively.\n",
      "The Age of Apocalypse version of Blink also visited it once, which proved that there's only one Negative Zone, as Annihilus remembered the Fantastic Four even though they never existed in the Age of Apocalypse.\n",
      "For a number of years, Captain Mar-Vell and Rick Jones were bonded to each other, causing one of them to exist in the Negative Zone while the other would exist in the regular universe.\n",
      "Spider-Man has also visited the Negative Zone, and acquired a costume that allowed him to merge with shadows and become practically invisible.\n",
      "When he was framed by Norman Osborn a few weeks later, he used the costume to become the dark, mysterious Dusk (one of his four new superhero identities during the Identity Crisis story arc).\n",
      "A few months later after Spider-Man's name was cleared, Cassie St. Commons was given the guise of Dusk and joined the Slingers.\n",
      "Cletus Kasady also visited the Negative Zone, finding and bonding with a symbiote there, as he had lost the original Carnage symbiote when Venom absorbed it into his own symbiote.\n",
      "The earliest origins of the Negative Zone and its culture have not been revealed, but rough estimates place its height of science and art over 1.5 million years ago, nearly coinciding with the rise of the Skrull and Kree races.\n",
      "It is believed that around that time the Negative Zone ceased expanding and began its \"Big Crunch\", contracting toward a central nexus.\n",
      "Some of the most powerful and influential races thereby faced destruction and sought to preserve their lives.\n",
      "One of the more aggressive races at this time were the Tyannans.\n",
      "The lion-like bipeds explored much of the Negative Zone and eventually began to seed many of the planets, including Baluur, with their \"spores of life\".\n",
      "However, their final mission went awry.\n",
      "A debris field had begun forming around Tyanna, which was at the very heart of the Negative Zone.\n",
      "The Big Crunch had begun pulling the universe back towards its center and planets crumbled under the increased pressure.\n",
      "One of the last remaining Tyannan ships careened off a large chunk of rock and crashed on the desolate planet of Arthros.\n",
      "The ship's engines dead and food processors destroyed, the Tyannan captain ordered the release of their life spores.\n",
      "Life slowly began to evolve on the planet, much as it did on Earth.\n",
      "Farther from the core of the Negative Zone, other cultures thrived.\n",
      "One culture had gone so far as to cover half of their planet with a giant city, and then developed an artificial brain for it.\n",
      "The city, Ootah, even began to develop a sense of self-preservation and drove the inhabitants out of its boundaries, building up greater defenses to prevent them from returning.\n",
      "Farther still from the core, the world of Kestor also flourished.\n",
      "It was the first recorded world beyond Tyanna that faced obliteration at the hands of the Big Crunch, roughly 10,000 years ago.\n",
      "Unlike Tyanna, whose gravitational force became greater than could sustain life, Kestor's sun was pulled toward the nexus.\n",
      "This caused the planet's several moons to shift their respective orbits and wreak havoc on the planet itself.\n",
      "As 20,000 beings left Kestor in a great space ark, the sun exploded with the fury of a supernova, substantially damaging their ship.\n",
      "All but 500 of the crew died and the navigational systems were destroyed, eliminating all hope of finding a new planet to call home.\n",
      "One of the planets the Tyannans spored, Baluur, developed in a more barbarous manner.\n",
      "The inhabitants grew very large and powerful, and even the meek among them were stronger than an ordinary human.\n",
      "While they did progress technologically, their advances were at least partially driven by war.\n",
      "But they apparently began to experience the effects of the Big Crunch and several millennia ago they took their cities underneath the planet's surface.\n",
      "A sole entrance exists and is known only to the Baluurians themselves.\n",
      "They established a quiet monarchy and became very reclusive, only venturing into space on rare occasions.\n",
      "Sentient life eventually developed on Arthros.\n",
      "A lone insect-like creature emerged from the primordial marsh one thousand years ago, and began to reason.\n",
      "A frail creature, he used his superior intellect to avoid larger predators and soon stumbled upon the Tyannan's derelict ship.\n",
      "Inside the craft, he donned a helmet in an attempt to warm himself.\n",
      "The helmet in fact had recordings of all the Tyannan technology and culture, and the creature was deftly able to assimilate all of it.\n",
      "He took the power from discarded life canisters and created a Cosmic Control Rod, capable of granting the wielder great power.\n",
      "Taking the name Annihilus, he set out to not only right those who wronged him, but to ensure that no one should ever harm him again.\n",
      "While Annihilus forcibly took control of his immediate sector of space, a king rose to prominence on Baluur.\n",
      "Blastaar was a ruthless and powerful leader who sought to expand Baluurian domain.\n",
      "The other Baluurians feared him and were eventually able to depose him by sedating him heavily and sending him to what they hoped would be his destruction in the center of the Negative Zone.\n",
      "While searching for a way to travel through sub-space, Reed Richards stumbled upon a gateway to the Negative Zone.\n",
      "He spent a fair amount of time studying it through probes, and determined that it was largely unpopulated.\n",
      "So much so, in fact, that he—and others—used the Negative Zone on several occasions to get rid of difficult enemies, such as the Mad Thinker's android, the Super-Adaptoid, and even Galactus.\n",
      "Reed and the Fantastic Four have since done more detailed explorations of the Zone and no longer use it to dispose of villains.\n",
      "For some time, the Norse realm of Asgard was lost in the Negative Zone.\n",
      "Civil War\n",
      "In the Super-Hero Civil War, a group of heroes led by Iron Man, Mister Fantastic and Yellowjacket have created a massive prison in the Negative Zone (similar to the Vault) to house captured non-registering heroes as they wait for their trials.\n",
      "It is designated Negative Zone Prison Alpha but nicknamed Fantasy Island by inmates.\n",
      "Tony Stark himself named it \"Project 42\", as it had been the 42nd idea out of a hundred that he, Reed Richards and Hank Pym had created following the Stamford Disaster.\n",
      "There are portals to it planned for every single state so prisoners can be transported there by the different teams in the Fifty State Initiative- including one at Ryker's Island.\n",
      "Its most notable former inmates were Iron Fist (posing as Daredevil at the time), Cloak and Dagger, Speedball, Prodigy and Prowler.\n",
      "It is also the setting for the final battle of the super hero civil war, as Iron Man lays a trap for Captain America but the Captain retaliates by using his planted mole, Hulkling, to release all the prisoners of 42 who come to his aid.\n",
      "Though in the ensuing battle Cloak manages to teleport all its superhuman detainees out, it remains as a prison for villains such as Lady Deathstrike and Taskmaster.\n",
      "(Taskmaster was later released to become the trainer for new Initiative recruits and Deathstrike seemingly escapes to assist in the Purifiers crusade during the Messiah Complex.)\n",
      "It also holds many of the Sakaaran forces from World War Hulk.\n",
      "In the one-shot Civil War: The Return, the Prison's warden was revealed as the first Captain Marvel, apparently back from the dead.\n",
      "Although he was later revealed as a Skrull sleeper agent named Khn'nr, whose conditioning was so strong he kept believing he was the Captain even after he had realized he really was not.\n",
      "Secret Invasion\n",
      "In the Secret Invasion: Fantastic Four limited series, the Skrull warrior Lyja (posing as Susan Richards) sends the Baxter Building into the Negative Zone.\n",
      "She reveals herself to Johnny Storm (her former spouse) and attacks him, feeling angry that he had forgotten her.\n",
      "During the course of their battle, Johnny saves Lyja from being hit by a police car, pulled in through the portal.\n",
      "The two reconcile after that, but a Negative Zone creature attacks them.\n",
      "They manage to defeat the creature, but Lyja passes out from her injuries.\n",
      "A bit later, when the \"new\" Fantastic Four fly off to the prison, Franklin and Valeria are grabbed by Negative Zone creatures, but Lyja saves them.\n",
      "Later when Ben, Johnny, Franklin, Val and the Tinkerer are ready to leave the Negative Zone she refuses to leave, because she wants to find out who she is.\n",
      "War of Kings\n",
      "After the events of Secret Invasion, the inmates of the prison took control of the facility after their correction officers abandoned the prison.\n",
      "Blastaar later overrun the prison during the prelude to the War of Kings story arc.\n",
      "Cataclysm\n",
      "The Negative Zone is used to dispose of Earth-616's Galactus when he is accidentally transferred to the Ultimate Marvel universe due to the temporal distortions caused by the events of Age of Ultron.\n",
      "The heroes of the Ultimate Marvel universe reasoned that Galactus will starve to death in the Negative Zone because it is a universe made of antimatter and Galactus would not have anything to eat there.\n",
      "Annihilation: Scourge\n",
      "The Negative Zone is later visited by the Sentry, who is struggling to understand his new state of self and find a way to separate himself from the Void.\n",
      "Upon soaking up the negative cosmic rays, the Sentry is able to separate himself from the Void but is left in the powerless form of Bob Reynolds.\n",
      "The Void was then drawn to the Cancerverse realm, where nothing could die, and impersonating and taking on the appearance of the Sentry, the Void became the new leader of its forces.\n",
      "He then led them back to the Negative Zone, forcing longtime enemies Blastaar and Annihilus to team up against them to prevent the Negative Zone kingdoms from being ravaged by the Undying Ones.\n",
      "Led by the Revengers, an ever-growing army of zombie-esque drones slaughter and infect their way from world to world giving no quarter as they leave destruction in their wake.\n",
      "As their desperate attempt to defend their domain fails, both rulers discover in the process that the Void is planning to conquer the Negative Zone and its denizens before returning to his own universe to spread the Cancerverse's infection even further.\n",
      "Annihilus then crosses into the positive matter universe to seek assistance, only to land right in the lap of Richard Ryder the last Nova, however, the past trauma at the hands of the Cancerverse, caused Ryder to flee, leaving Annihilus at the mercy of his enemies.\n",
      "The Void then attempt to cross over into the positive Universe, but was stopped by Beta Ray Bill and Lockjaw, with Beta Ray Bill forced to sacrifice his magical hammer, Stormbreaker, in the process.\n",
      "The Crossroads of Infinity\n",
      "Initially, it was generally believed that getting caught in the gravitational pull of the vortex at the center of the Zone meant certain death.\n",
      "However, some theorized that if one could survive entry into the vortex, one could travel to another dimension.\n",
      "However, this was only a theory until Doctor Doom gambled the lives of the Fantastic Four to prove it.\n",
      "Once inside the so-called Crossroads, individuals flip through a series of other parallel dimensions while progressing through.\n",
      "Coming to a halt seems to stabilize the Crossroads and then allows movement in the universe arrived in.\n",
      "However, it still remains a risky trip for a number of reasons.\n",
      "First and foremost is the expenditure of energy necessary to keep from imploding.\n",
      "On one occasion, it required the use of both Annihilus' Cosmic Control Rod and the Invisible Woman's force field to stave off death.\n",
      "Second, and almost equally important, is the lack of control one has while passing through different dimensions.\n",
      "Even with the precise calculations of Dr. Doom, it required several hops to reach the dimension he sought.\n",
      "Staying on a straight course to the very heart of the Crossroads leads one to Tyanna.\n",
      "Although it was believed to have been destroyed centuries earlier, the Tyannans' technology proved capable of sustaining them within the center of the vortex.\n",
      "Using their advanced machines, they were able to modify their bodies to exist safely in the immense pressures, but at the cost of being forced to remain hidden within the Crossroads.\n",
      "The Tyannans remain there, content to pursue their scientific interests.\n",
      "The Distortion Area\n",
      "Normally beings enter the Negative Zone through the Distortion Area.\n",
      "This is an invisible sphere of energy that resides in the Negative Zone but is accessible from many parts of Earth.\n",
      "By hitting the field with a precise wavelength of energy, a rift opens between both dimensions connected by the Distortion Area.\n",
      "This area acts as a buffer between the two polar opposite universes and alters a traveler's own polarity so that they may exist in the other dimension without harm.\n",
      "When activated, the Distortion Area appears from the outside as a crackling energy source roughly six feet in circumference.\n",
      "Nearby matter is sucked into the near-vacuum of the Distortion Area and \"falls\" for about 50 seconds before emerging on the other side.\n",
      "The Distortion Area itself is nothing short of indescribable.\n",
      "Humans cannot begin to accurately fathom or record what transpires within the Distortion Area and travelers' minds try to compensate with a bizarre display of light and color.\n",
      "Combined with the natural turbulence in the area, many find the trip rather nauseating.\n",
      "Like any other mode of transport, choosing where to enter the Distortion Area in part affects where an individual is deposited on the other side.\n",
      "Reed Richards' former lab in the Baxter Building, for example, deposited someone at the outskirts of the Debris Field near Arthros.\n",
      "A rift opened on Yancy Street, however, dropped a traveler on the planet Tarsuu.\n",
      "Other methods may be employed to reach the Negative Zone but, while they tend to be more spatially accurate, they are very difficult to come by.\n",
      "Generally, this mode of transport is reserved by extremely powerful entities like Thor, the Supreme Intelligence, Galactus and the Watchers.\n",
      "Captain Mar-Vell's Nega-Bands could also transport the wearer to the Negative Zone but the precise destination was dependent upon the location of the Nega-Band's counterparts worn by Rick Jones.\n",
      "Emotions\n",
      "One of the rather undocumented side-effects the Negative Zone has on people seems to involve bringing forth what are usually considered negative emotions.\n",
      "At this point, it is difficult to determine the cause of this unusual phenomenon, but a number of dealings with the Negative Zone have given the impression that something inherent within it can lead to emotional distress.\n",
      "The severity of this distress can vary greatly.\n",
      "On a few occasions, it has manifested itself as a citywide panic.\n",
      "Other times, it merely caused a single person to temporarily lose their hope.\n",
      "Still other times, it caused several individuals to experience emotionally charged flashbacks, pulled from their subconscious.\n",
      "And, although it could be attributed to his psyche, Annihilus' main motivation is an obsessive fear of death.\n",
      "Not all inhabitants of the Negative Zone experience anything beyond normal anxieties, but the number of known instances of unusual psychological distress seems rather high.\n",
      "Visitors to the Zone may or may not experience anything emotionally disturbing.\n",
      "While there is no concrete evidence to show the correlation the Negative Zone has with emotional discord, it remains a significant feature of the region.\n",
      "As mentioned previously, the Negative Zone was originally thought to be largely uninhabited.\n",
      "Though this is now known to be incorrect, life having arisen on a number of its planets, the Zone is still mostly unoccupied space.\n",
      "Most of the encounters heroes have had with the Negative Zone revolve around Annihilus and/or Blastaar.\n",
      "Curiously, for as little life as there exists in the Negative Zone, it is very capable of supporting life.\n",
      "There, outer space itself is permeated with an oxygen-rich atmosphere, closely approximating Earth's.\n",
      "Consequently, humans can exist peacefully in space without the need for cumbersome pressure suits or oxygen masks.\n",
      "It is perhaps an issue of gravitational pull that is one of the biggest hindrances to life in the Negative Zone.\n",
      "While all objects of reasonably sized mass (planets, moons, asteroids, etc.) obviously have their own gravitational pull, it is weak enough to be overcome with minimal effort.\n",
      "Most heroes with flight capabilities can escape a planet's gravitational field with ease, as can any machine with the capacity for flight.\n",
      "Because of this lowered gravity, it is believed that vegetation has difficulty seeding properly, giving life a tenuous foothold at best on any given planet.\n",
      "Moisture also seems to be an issue in the dearth of life in the Negative Zone.\n",
      "While each planet has hardly been explored in full, many of the ones studied—including those that are inhabited—have shown few, if any, natural water sources.\n",
      "Most planets appear to be very arid, with cultures adapting to the lack of moisture much as was done on Earth's deserts.\n",
      "Being a different dimension with different laws of physics, time flows differently in the Negative Zone than it does on Earth.\n",
      "Although a comprehensive analysis has never been completed, preliminary findings suggest that two weeks pass in the Negative Zone for every hour on Earth, making a 336:1 ratio.\n",
      "On a smaller scale, every minute on Earth is a little over five and half hours in the Negative Zone.\n",
      "It seems, however, that this ratio changes as one approaches the center of the Negative Zone.\n",
      "Since there have been several instances of pan-dimensional conversations between the two planes, it appears the time ratio is much closer to 1:1 when one is at the nexus of the Negative Zone.\n",
      "It is currently unclear at what rate the time difference increases as one moves away from the vortex or if there is an upper limit to how great the difference between the two universes can become.\n",
      "Planets\n",
      "The Negative Zone includes the following planets:\n",
      "Argor – A planet that is home to the Argorans.\n",
      "Arthros – A planet that is home to Annihilus and the Arthosians (a race of insectoids).\n",
      "Baluur – A planet that is home to Blastaar.\n",
      "Kestor – A planet that was destroyed causing the Kestorians to become nomads.\n",
      "Tyanna – A planet that is home to the Tyannans (a race of lion-like aliens).\n",
      "Tyanna is located at the Crossroads of Infinity.\n",
      "The Tyannan scientists genetically engineered a spore that could be scattered over an uninhabited planet's surface where they can grow into new plants and animals.\n",
      "This terraforming made the planet habitable.\n",
      "In the Blink miniseries, it is established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Blink from the Age of Apocalypse found herself in a Negative Zone that still remembered the Fantastic Four's role in defeating Annihilus despite the fact that the team never existed in her world.\n",
      "This goes along with the Mutant X Universe, where Havok tried to get back from the Mutant X-verse to the main Marvel earth by a detour through the Negative Zone.\n",
      "Additionally both Rikki Barnes and Onslaught traveled from the Pocket Universe of Heroes Reborn to the mainstream Marvel Universe through the Negative Zone.\n",
      "During the Cataclysm event it is again established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Ultimate Marvel heroes, after a brief trip to Earth-616 to acquire local information on Galactus, eventually manage to send Galactus to the Negative Zone, reasoning that he will eventually starve to death because Negative Zone is made of anti-matter, however, following an incident on Earth 616 where the Eternal known as Ikaris is brainwashed by a Kree device called the God's Whisper, the Eternals retrieve, with help from Aarkus, the comatose Galactus from the Negative Zone, and state that they plan to use the God's Whisper to unleash him upon the Kree when he awakens as revenge for what they did to Ikaris.\n",
      "Ultimate Marvel (The N-Zone)\n",
      "The Ultimate Marvel equivalent of the Negative Zone was originally called the N-Zone, a zone that exists directly below the Ultimate Marvel universe.\n",
      "It is tied to the powers granted to the Fantastic Four and is the homeplace of the Ultimate Fantastic Four villain Nihil.\n",
      "The N-Zone is a universe in the later stages of entropic heat death, with less than a million years of existence left to it.\n",
      "There are few stars still burning, mostly long-lived red dwarf stars, and despite the advanced technology of many races, life barely maintains a toehold on its existence.\n",
      "Space is also apparently foreshortened in this universe, with the Fantastic Four's shuttle Awesome achieving speeds that would be impossible in our own universe.\n",
      "It has an atmosphere that is lethally acidic to humans and the Ultimate Fantastic Four, with the exception of the Thing, required space suits to live.\n",
      "Later it was reckoned that the N-Zone is one of many other zones, labeled with letters (e.g. the Z-Zone and the Q-Zone) implying that the N is merely a categorization, not a shortening of the word \"negative\" therefore no logger connected to the Negative Zone.\n",
      "A proper Negative Zone was eventually introduced in the Ultimate Marvel where The Ultimates confront Reed Richards.\n",
      "Heroes Reborn (2021)\n",
      "In an alternate reality depicted in the 2021 \"Heroes Reborn\" miniseries, the Negative Zone is used by the Squadron Supreme of America to imprison Earth's most dangerous villains, such as General Annihilus, Doctor Juggernaut, the Hulk, Mister Beyonder, Namor, and Hank Pym / Ultron.\n",
      "The Negative Zone appears in the 1994 Fantastic Four animated series episode, \"Behold the Negative Zone\".\n",
      "In a scripted, but unproduced episode of The Silver Surfer titled \"Down to Earth\" Part 3, Reed Richards uses the Negative Zone as a means to contain Terrax.\n",
      "The Negative Zone appears in Fantastic Four: World's Greatest Heroes.\n",
      "This version is depicted as an alternate dimension inhabited by an array of various serpentine and insectoid creatures.\n",
      "According to Doctor Doom, the Negative Zone is a nexus for various other universes and claims that he discovered it years before Reed Richards.\n",
      "The Negative Zone appears in the two-part Iron Man: Armored Adventures episode \"The Makluan Invasion\".\n",
      "It is depicted as a dark empty void which the Mandarin banished S.H.I.E.L.D.'s Helicarrier to before rescuing it at the end of the episode following the titular event.\n",
      "The Negative Zone appears in The Avengers: Earth's Mightiest Heroes.\n",
      "It debuts in the episode \"The Man Who Stole Tomorrow\", when Thor, Ant-Man, and the Wasp place Blizzard in Negative Zone Prison 42.\n",
      "In the episode \"Assault on 42\", Annihilus and his insectoid slaves attack Prison 42, but were defeated by the joint forces of Captain America, Thor, Wasp, Miss Marvel, and the imprisoned supervillains of Prison 42.\n",
      "In the episode \"Avengers Assemble\", Mister Fantastic and Iron Man create a portal to the Negative Zone to send Galactus there to feed on its infinite anti-matter instead of Earth, using Galactus's own emitted energy as a tractor beam.\n",
      "The Negative Zone appears in the Hulk and the Agents of S.M.A.S.H. episodes \"Doorway to Destruction\", \"Into the Negative Zone\", and \"Days of Future Smash\" Pt. 5.\n",
      "In the original script for the Fantastic Four reboot by Jeremy Slater, the Negative Zone was reinvented as a planet that was home to ancient alien civilization before being destroyed by Galactus.\n",
      "However, after re-shoots and edits, the Negative Zone was renamed Planet Zero, and reworked into a planet with a cracked landscape and unstable energy that becomes the source of the titular characters' and Victor von Doom's mutations.\n",
      "The Negative Zone appears in Marvel: Ultimate Alliance 2.\n",
      "It contains \"Prison 42\", a prison for unregistered heroes and nanite-controlled supervillains, and where the Fold first becomes active before Nick Fury destroyed it to keep the Fold from invading Earth.\n",
      "The Negative Zone appears in the Marvel Super Hero Squad: The Infinity Gauntlet video game.\n",
      "Phantom Zone\n",
      "Negative Zone at Marvel.com\n",
      "Negative Zone at Marvel Wiki\n",
      "Marvel Comics dimensions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_preserve(doc_mldr, doc_id=\"doc0\", min_samples=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constrained single-linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|0 The Negative Zone is a fictional setting, an antimatter universe appearing in American comic books published by Marvel Comics.\n",
      "A few notable residents of the Negative Zone include Blastaar and Annihilus.\n",
      "The Negative Zone is often visited by the Fantastic Four as Mister Fantastic discovered it, and has mapped portions of it extensively.\n",
      "The Age of Apocalypse version of Blink also visited it once, which proved that there's only one Negative Zone, as Annihilus remembered the Fantastic Four even though they never existed in the Age of Apocalypse.\n",
      "The Negative Zone is used to dispose of Earth-616's Galactus when he is accidentally transferred to the Ultimate Marvel universe due to the temporal distortions caused by the events of Age of Ultron.\n",
      "The heroes of the Ultimate Marvel universe reasoned that Galactus will starve to death in the Negative Zone because it is a universe made of antimatter and Galactus would not have anything to eat there.\n",
      "Most of the encounters heroes have had with the Negative Zone revolve around Annihilus and/or Blastaar.\n",
      "In the Blink miniseries, it is established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Blink from the Age of Apocalypse found herself in a Negative Zone that still remembered the Fantastic Four's role in defeating Annihilus despite the fact that the team never existed in her world.\n",
      "The Ultimate Marvel equivalent of the Negative Zone was originally called the N-Zone, a zone that exists directly below the Ultimate Marvel universe.\n",
      "A proper Negative Zone was eventually introduced in the Ultimate Marvel where The Ultimates confront Reed Richards.\n",
      "The Negative Zone appears in the 1994 Fantastic Four animated series episode, \"Behold the Negative Zone\".\n",
      "The Negative Zone appears in Fantastic Four: World's Greatest Heroes.\n",
      "The Negative Zone appears in the two-part Iron Man: Armored Adventures episode \"The Makluan Invasion\".\n",
      "The Negative Zone appears in The Avengers: Earth's Mightiest Heroes.\n",
      "In the episode \"Avengers Assemble\", Mister Fantastic and Iron Man create a portal to the Negative Zone to send Galactus there to feed on its infinite anti-matter instead of Earth, using Galactus's own emitted energy as a tractor beam.\n",
      "The Negative Zone appears in the Hulk and the Agents of S.M.A.S.H. episodes \"Doorway to Destruction\", \"Into the Negative Zone\", and \"Days of Future Smash\" Pt. 5.\n",
      "In the original script for the Fantastic Four reboot by Jeremy Slater, the Negative Zone was reinvented as a planet that was home to ancient alien civilization before being destroyed by Galactus.\n",
      "The Negative Zone appears in Marvel: Ultimate Alliance 2.\n",
      "The Negative Zone appears in the Marvel Super Hero Squad: The Infinity Gauntlet video game.\n",
      "Negative Zone at Marvel.com\n",
      "Negative Zone at Marvel Wiki\n",
      "\n",
      "doc0|1 The location is depicted in various publications from Marvel, most frequently in Fantastic Four and Captain Marvel.\n",
      "Created by Stan Lee and Jack Kirby, it first appeared in Fantastic Four #51 (June 1966).\n",
      "Reed and the Fantastic Four have since done more detailed explorations of the Zone and no longer use it to dispose of villains.\n",
      "It also holds many of the Sakaaran forces from World War Hulk.\n",
      "In the Secret Invasion: Fantastic Four limited series, the Skrull warrior Lyja (posing as Susan Richards) sends the Baxter Building into the Negative Zone.\n",
      "She reveals herself to Johnny Storm (her former spouse) and attacks him, feeling angry that he had forgotten her.\n",
      "During the course of their battle, Johnny saves Lyja from being hit by a police car, pulled in through the portal.\n",
      "They manage to defeat the creature, but Lyja passes out from her injuries.\n",
      "A bit later, when the \"new\" Fantastic Four fly off to the prison, Franklin and Valeria are grabbed by Negative Zone creatures, but Lyja saves them.\n",
      "Later when Ben, Johnny, Franklin, Val and the Tinkerer are ready to leave the Negative Zone she refuses to leave, because she wants to find out who she is.\n",
      "However, this was only a theory until Doctor Doom gambled the lives of the Fantastic Four to prove it.\n",
      "This is an invisible sphere of energy that resides in the Negative Zone but is accessible from many parts of Earth.\n",
      "Reed Richards' former lab in the Baxter Building, for example, deposited someone at the outskirts of the Debris Field near Arthros.\n",
      "There, outer space itself is permeated with an oxygen-rich atmosphere, closely approximating Earth's.\n",
      "Consequently, humans can exist peacefully in space without the need for cumbersome pressure suits or oxygen masks.\n",
      "It is tied to the powers granted to the Fantastic Four and is the homeplace of the Ultimate Fantastic Four villain Nihil.\n",
      "The N-Zone is a universe in the later stages of entropic heat death, with less than a million years of existence left to it.\n",
      "Space is also apparently foreshortened in this universe, with the Fantastic Four's shuttle Awesome achieving speeds that would be impossible in our own universe.\n",
      "It has an atmosphere that is lethally acidic to humans and the Ultimate Fantastic Four, with the exception of the Thing, required space suits to live.\n",
      "It is depicted as a dark empty void which the Mandarin banished S.H.I.E.L.D.'s Helicarrier to before rescuing it at the end of the episode following the titular event.\n",
      "Phantom Zone\n",
      "\n",
      "doc0|2 Fictional description\n",
      "They exchanged places by clasping the special bracelets each wore or automatically after a few hours.\n",
      "A sole entrance exists and is known only to the Baluurians themselves.\n",
      "It is very clean, with sanitation, but extremely heavily guarded, including password-changes every ten minutes.\n",
      "However, it still remains a risky trip for a number of reasons.\n",
      "First and foremost is the expenditure of energy necessary to keep from imploding.\n",
      "Second, and almost equally important, is the lack of control one has while passing through different dimensions.\n",
      "Even with the precise calculations of Dr. Doom, it required several hops to reach the dimension he sought.\n",
      "The Distortion Area\n",
      "By hitting the field with a precise wavelength of energy, a rift opens between both dimensions connected by the Distortion Area.\n",
      "When activated, the Distortion Area appears from the outside as a crackling energy source roughly six feet in circumference.\n",
      "This effect only lasts as long as the field is activated and, once closed, becomes invisible again.\n",
      "Nearby matter is sucked into the near-vacuum of the Distortion Area and \"falls\" for about 50 seconds before emerging on the other side.\n",
      "The Distortion Area itself is nothing short of indescribable.\n",
      "Humans cannot begin to accurately fathom or record what transpires within the Distortion Area and travelers' minds try to compensate with a bizarre display of light and color.\n",
      "Combined with the natural turbulence in the area, many find the trip rather nauseating.\n",
      "Like any other mode of transport, choosing where to enter the Distortion Area in part affects where an individual is deposited on the other side.\n",
      "A rift opened on Yancy Street, however, dropped a traveler on the planet Tarsuu.\n",
      "Tarsuu –\n",
      "Other versions\n",
      "This version is depicted as an alternate dimension inhabited by an array of various serpentine and insectoid creatures.\n",
      "\n",
      "doc0|3 The Negative Zone in the Marvel Universe is used as a fictional dimension.\n",
      "For a number of years, Captain Mar-Vell and Rick Jones were bonded to each other, causing one of them to exist in the Negative Zone while the other would exist in the regular universe.\n",
      "While searching for a way to travel through sub-space, Reed Richards stumbled upon a gateway to the Negative Zone.\n",
      "In the Super-Hero Civil War, a group of heroes led by Iron Man, Mister Fantastic and Yellowjacket have created a massive prison in the Negative Zone (similar to the Vault) to house captured non-registering heroes as they wait for their trials.\n",
      "It is designated Negative Zone Prison Alpha but nicknamed Fantasy Island by inmates.\n",
      "There are portals to it planned for every single state so prisoners can be transported there by the different teams in the Fifty State Initiative- including one at Ryker's Island.\n",
      "It is also the setting for the final battle of the super hero civil war, as Iron Man lays a trap for Captain America but the Captain retaliates by using his planted mole, Hulkling, to release all the prisoners of 42 who come to his aid.\n",
      "Being a different dimension with different laws of physics, time flows differently in the Negative Zone than it does on Earth.\n",
      "Although a comprehensive analysis has never been completed, preliminary findings suggest that two weeks pass in the Negative Zone for every hour on Earth, making a 336:1 ratio.\n",
      "On a smaller scale, every minute on Earth is a little over five and half hours in the Negative Zone.\n",
      "This goes along with the Mutant X Universe, where Havok tried to get back from the Mutant X-verse to the main Marvel earth by a detour through the Negative Zone.\n",
      "Additionally both Rikki Barnes and Onslaught traveled from the Pocket Universe of Heroes Reborn to the mainstream Marvel Universe through the Negative Zone.\n",
      "During the Cataclysm event it is again established that there is in fact only one Negative Zone in the multiverse of Marvel Universe, with exit points to different realities and timelines, as the Ultimate Marvel heroes, after a brief trip to Earth-616 to acquire local information on Galactus, eventually manage to send Galactus to the Negative Zone, reasoning that he will eventually starve to death because Negative Zone is made of anti-matter, however, following an incident on Earth 616 where the Eternal known as Ikaris is brainwashed by a Kree device called the God's Whisper, the Eternals retrieve, with help from Aarkus, the comatose Galactus from the Negative Zone, and state that they plan to use the God's Whisper to unleash him upon the Kree when he awakens as revenge for what they did to Ikaris.\n",
      "Ultimate Marvel (The N-Zone)\n",
      "In an alternate reality depicted in the 2021 \"Heroes Reborn\" miniseries, the Negative Zone is used by the Squadron Supreme of America to imprison Earth's most dangerous villains, such as General Annihilus, Doctor Juggernaut, the Hulk, Mister Beyonder, Namor, and Hank Pym / Ultron.\n",
      "In a scripted, but unproduced episode of The Silver Surfer titled \"Down to Earth\" Part 3, Reed Richards uses the Negative Zone as a means to contain Terrax.\n",
      "According to Doctor Doom, the Negative Zone is a nexus for various other universes and claims that he discovered it years before Reed Richards.\n",
      "In the episode \"Assault on 42\", Annihilus and his insectoid slaves attack Prison 42, but were defeated by the joint forces of Captain America, Thor, Wasp, Miss Marvel, and the imprisoned supervillains of Prison 42.\n",
      "However, after re-shoots and edits, the Negative Zone was renamed Planet Zero, and reworked into a planet with a cracked landscape and unstable energy that becomes the source of the titular characters' and Victor von Doom's mutations.\n",
      "It contains \"Prison 42\", a prison for unregistered heroes and nanite-controlled supervillains, and where the Fold first becomes active before Nick Fury destroyed it to keep the Fold from invading Earth.\n",
      "Marvel Comics dimensions\n",
      "\n",
      "doc0|4 Essentially, it is a universe parallel to Earth's.\n",
      "The Big Crunch had begun pulling the universe back towards its center and planets crumbled under the increased pressure.\n",
      "Life slowly began to evolve on the planet, much as it did on Earth.\n",
      "It was the first recorded world beyond Tyanna that faced obliteration at the hands of the Big Crunch, roughly 10,000 years ago.\n",
      "This caused the planet's several moons to shift their respective orbits and wreak havoc on the planet itself.\n",
      "But they apparently began to experience the effects of the Big Crunch and several millennia ago they took their cities underneath the planet's surface.\n",
      "Sentient life eventually developed on Arthros.\n",
      "The Crossroads of Infinity\n",
      "However, some theorized that if one could survive entry into the vortex, one could travel to another dimension.\n",
      "Once inside the so-called Crossroads, individuals flip through a series of other parallel dimensions while progressing through.\n",
      "Coming to a halt seems to stabilize the Crossroads and then allows movement in the universe arrived in.\n",
      "Staying on a straight course to the very heart of the Crossroads leads one to Tyanna.\n",
      "This area acts as a buffer between the two polar opposite universes and alters a traveler's own polarity so that they may exist in the other dimension without harm.\n",
      "Because of this lowered gravity, it is believed that vegetation has difficulty seeding properly, giving life a tenuous foothold at best on any given planet.\n",
      "While each planet has hardly been explored in full, many of the ones studied—including those that are inhabited—have shown few, if any, natural water sources.\n",
      "Most planets appear to be very arid, with cultures adapting to the lack of moisture much as was done on Earth's deserts.\n",
      "Argor – A planet that is home to the Argorans.\n",
      "Arthros – A planet that is home to Annihilus and the Arthosians (a race of insectoids).\n",
      "Tyanna – A planet that is home to the Tyannans (a race of lion-like aliens).\n",
      "Tyanna is located at the Crossroads of Infinity.\n",
      "This terraforming made the planet habitable.\n",
      "\n",
      "doc0|5 The two have many similarities, but a few noteworthy differences include: all matter in the Negative Zone is negatively charged; the Negative Zone is entirely filled with a pressurized, breathable atmosphere; and near the center of the Negative Zone is a deadly vortex of unspeakable power.\n",
      "Spider-Man has also visited the Negative Zone, and acquired a costume that allowed him to merge with shadows and become practically invisible.\n",
      "When he was framed by Norman Osborn a few weeks later, he used the costume to become the dark, mysterious Dusk (one of his four new superhero identities during the Identity Crisis story arc).\n",
      "A few months later after Spider-Man's name was cleared, Cassie St. Commons was given the guise of Dusk and joined the Slingers.\n",
      "Cletus Kasady also visited the Negative Zone, finding and bonding with a symbiote there, as he had lost the original Carnage symbiote when Venom absorbed it into his own symbiote.\n",
      "A debris field had begun forming around Tyanna, which was at the very heart of the Negative Zone.\n",
      "Inside the craft, he donned a helmet in an attempt to warm himself.\n",
      "He took the power from discarded life canisters and created a Cosmic Control Rod, capable of granting the wielder great power.\n",
      "Its most notable former inmates were Iron Fist (posing as Daredevil at the time), Cloak and Dagger, Speedball, Prodigy and Prowler.\n",
      "Though in the ensuing battle Cloak manages to teleport all its superhuman detainees out, it remains as a prison for villains such as Lady Deathstrike and Taskmaster.\n",
      "(Taskmaster was later released to become the trainer for new Initiative recruits and Deathstrike seemingly escapes to assist in the Purifiers crusade during the Messiah Complex.)\n",
      "In the one-shot Civil War: The Return, the Prison's warden was revealed as the first Captain Marvel, apparently back from the dead.\n",
      "Although he was later revealed as a Skrull sleeper agent named Khn'nr, whose conditioning was so strong he kept believing he was the Captain even after he had realized he really was not.\n",
      "Cataclysm\n",
      "Initially, it was generally believed that getting caught in the gravitational pull of the vortex at the center of the Zone meant certain death.\n",
      "On one occasion, it required the use of both Annihilus' Cosmic Control Rod and the Invisible Woman's force field to stave off death.\n",
      "Captain Mar-Vell's Nega-Bands could also transport the wearer to the Negative Zone but the precise destination was dependent upon the location of the Nega-Band's counterparts worn by Rick Jones.\n",
      "The severity of this distress can vary greatly.\n",
      "On a few occasions, it has manifested itself as a citywide panic.\n",
      "Other times, it merely caused a single person to temporarily lose their hope.\n",
      "Still other times, it caused several individuals to experience emotionally charged flashbacks, pulled from their subconscious.\n",
      "\n",
      "doc0|6 Since the Negative Zone is largely uninhabited, several would-be conquerors have attempted to bridge the gap to Earth and take over its population.\n",
      "The earliest origins of the Negative Zone and its culture have not been revealed, but rough estimates place its height of science and art over 1.5 million years ago, nearly coinciding with the rise of the Skrull and Kree races.\n",
      "It is believed that around that time the Negative Zone ceased expanding and began its \"Big Crunch\", contracting toward a central nexus.\n",
      "Farther from the core of the Negative Zone, other cultures thrived.\n",
      "For some time, the Norse realm of Asgard was lost in the Negative Zone.\n",
      "The Negative Zone is later visited by the Sentry, who is struggling to understand his new state of self and find a way to separate himself from the Void.\n",
      "Upon soaking up the negative cosmic rays, the Sentry is able to separate himself from the Void but is left in the powerless form of Bob Reynolds.\n",
      "Normally beings enter the Negative Zone through the Distortion Area.\n",
      "Other methods may be employed to reach the Negative Zone but, while they tend to be more spatially accurate, they are very difficult to come by.\n",
      "One of the rather undocumented side-effects the Negative Zone has on people seems to involve bringing forth what are usually considered negative emotions.\n",
      "At this point, it is difficult to determine the cause of this unusual phenomenon, but a number of dealings with the Negative Zone have given the impression that something inherent within it can lead to emotional distress.\n",
      "Not all inhabitants of the Negative Zone experience anything beyond normal anxieties, but the number of known instances of unusual psychological distress seems rather high.\n",
      "Visitors to the Zone may or may not experience anything emotionally disturbing.\n",
      "While there is no concrete evidence to show the correlation the Negative Zone has with emotional discord, it remains a significant feature of the region.\n",
      "As mentioned previously, the Negative Zone was originally thought to be largely uninhabited.\n",
      "Though this is now known to be incorrect, life having arisen on a number of its planets, the Zone is still mostly unoccupied space.\n",
      "Curiously, for as little life as there exists in the Negative Zone, it is very capable of supporting life.\n",
      "It is perhaps an issue of gravitational pull that is one of the biggest hindrances to life in the Negative Zone.\n",
      "Moisture also seems to be an issue in the dearth of life in the Negative Zone.\n",
      "The Negative Zone includes the following planets:\n",
      "Later it was reckoned that the N-Zone is one of many other zones, labeled with letters (e.g. the Z-Zone and the Q-Zone) implying that the N is merely a categorization, not a shortening of the word \"negative\" therefore no logger connected to the Negative Zone.\n",
      "\n",
      "doc0|7 History\n",
      "The helmet in fact had recordings of all the Tyannan technology and culture, and the creature was deftly able to assimilate all of it.\n",
      "Discovery\n",
      "Civil War\n",
      "Secret Invasion\n",
      "War of Kings\n",
      "Led by the Revengers, an ever-growing army of zombie-esque drones slaughter and infect their way from world to world giving no quarter as they leave destruction in their wake.\n",
      "Unique features\n",
      "Emotions\n",
      "Time\n",
      "The Tyannan scientists genetically engineered a spore that could be scattered over an uninhabited planet's surface where they can grow into new plants and animals.\n",
      "In other media\n",
      "Television\n",
      "Film\n",
      "Video games\n",
      "See also\n",
      "References\n",
      "External links\n",
      "\n",
      "doc0|8 Some of the most powerful and influential races thereby faced destruction and sought to preserve their lives.\n",
      "One of the more aggressive races at this time were the Tyannans.\n",
      "One culture had gone so far as to cover half of their planet with a giant city, and then developed an artificial brain for it.\n",
      "The city, Ootah, even began to develop a sense of self-preservation and drove the inhabitants out of its boundaries, building up greater defenses to prevent them from returning.\n",
      "The inhabitants grew very large and powerful, and even the meek among them were stronger than an ordinary human.\n",
      "While they did progress technologically, their advances were at least partially driven by war.\n",
      "They established a quiet monarchy and became very reclusive, only venturing into space on rare occasions.\n",
      "A lone insect-like creature emerged from the primordial marsh one thousand years ago, and began to reason.\n",
      "Taking the name Annihilus, he set out to not only right those who wronged him, but to ensure that no one should ever harm him again.\n",
      "He spent a fair amount of time studying it through probes, and determined that it was largely unpopulated.\n",
      "The two reconcile after that, but a Negative Zone creature attacks them.\n",
      "Annihilation: Scourge\n",
      "The Void was then drawn to the Cancerverse realm, where nothing could die, and impersonating and taking on the appearance of the Sentry, the Void became the new leader of its forces.\n",
      "As their desperate attempt to defend their domain fails, both rulers discover in the process that the Void is planning to conquer the Negative Zone and its denizens before returning to his own universe to spread the Cancerverse's infection even further.\n",
      "Annihilus then crosses into the positive matter universe to seek assistance, only to land right in the lap of Richard Ryder the last Nova, however, the past trauma at the hands of the Cancerverse, caused Ryder to flee, leaving Annihilus at the mercy of his enemies.\n",
      "The Void then attempt to cross over into the positive Universe, but was stopped by Beta Ray Bill and Lockjaw, with Beta Ray Bill forced to sacrifice his magical hammer, Stormbreaker, in the process.\n",
      "Using their advanced machines, they were able to modify their bodies to exist safely in the immense pressures, but at the cost of being forced to remain hidden within the Crossroads.\n",
      "And, although it could be attributed to his psyche, Annihilus' main motivation is an obsessive fear of death.\n",
      "It seems, however, that this ratio changes as one approaches the center of the Negative Zone.\n",
      "Since there have been several instances of pan-dimensional conversations between the two planes, it appears the time ratio is much closer to 1:1 when one is at the nexus of the Negative Zone.\n",
      "It is currently unclear at what rate the time difference increases as one moves away from the vortex or if there is an upper limit to how great the difference between the two universes can become.\n",
      "\n",
      "doc0|9 The lion-like bipeds explored much of the Negative Zone and eventually began to seed many of the planets, including Baluur, with their \"spores of life\".\n",
      "However, their final mission went awry.\n",
      "One of the last remaining Tyannan ships careened off a large chunk of rock and crashed on the desolate planet of Arthros.\n",
      "The ship's engines dead and food processors destroyed, the Tyannan captain ordered the release of their life spores.\n",
      "Farther still from the core, the world of Kestor also flourished.\n",
      "Unlike Tyanna, whose gravitational force became greater than could sustain life, Kestor's sun was pulled toward the nexus.\n",
      "As 20,000 beings left Kestor in a great space ark, the sun exploded with the fury of a supernova, substantially damaging their ship.\n",
      "All but 500 of the crew died and the navigational systems were destroyed, eliminating all hope of finding a new planet to call home.\n",
      "One of the planets the Tyannans spored, Baluur, developed in a more barbarous manner.\n",
      "A frail creature, he used his superior intellect to avoid larger predators and soon stumbled upon the Tyannan's derelict ship.\n",
      "While Annihilus forcibly took control of his immediate sector of space, a king rose to prominence on Baluur.\n",
      "Blastaar was a ruthless and powerful leader who sought to expand Baluurian domain.\n",
      "The other Baluurians feared him and were eventually able to depose him by sedating him heavily and sending him to what they hoped would be his destruction in the center of the Negative Zone.\n",
      "So much so, in fact, that he—and others—used the Negative Zone on several occasions to get rid of difficult enemies, such as the Mad Thinker's android, the Super-Adaptoid, and even Galactus.\n",
      "After the events of Secret Invasion, the inmates of the prison took control of the facility after their correction officers abandoned the prison.\n",
      "Blastaar later overrun the prison during the prelude to the War of Kings story arc.\n",
      "He then led them back to the Negative Zone, forcing longtime enemies Blastaar and Annihilus to team up against them to prevent the Negative Zone kingdoms from being ravaged by the Undying Ones.\n",
      "Although it was believed to have been destroyed centuries earlier, the Tyannans' technology proved capable of sustaining them within the center of the vortex.\n",
      "The Tyannans remain there, content to pursue their scientific interests.\n",
      "Baluur – A planet that is home to Blastaar.\n",
      "Kestor – A planet that was destroyed causing the Kestorians to become nomads.\n",
      "\n",
      "doc0|10 Tony Stark himself named it \"Project 42\", as it had been the 42nd idea out of a hundred that he, Reed Richards and Hank Pym had created following the Stamford Disaster.\n",
      "Generally, this mode of transport is reserved by extremely powerful entities like Thor, the Supreme Intelligence, Galactus and the Watchers.\n",
      "Life\n",
      "While all objects of reasonably sized mass (planets, moons, asteroids, etc.) obviously have their own gravitational pull, it is weak enough to be overcome with minimal effort.\n",
      "Most heroes with flight capabilities can escape a planet's gravitational field with ease, as can any machine with the capacity for flight.\n",
      "Planets\n",
      "There are few stars still burning, mostly long-lived red dwarf stars, and despite the advanced technology of many races, life barely maintains a toehold on its existence.\n",
      "Heroes Reborn (2021)\n",
      "It debuts in the episode \"The Man Who Stole Tomorrow\", when Thor, Ant-Man, and the Wasp place Blizzard in Negative Zone Prison 42.\n",
      "Qward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker(doc_mldr, doc_id=\"doc0\", encoder=get_encoder(\"huggingface\"))\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: BEIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25657/25657 [00:00<00:00, 47568.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crowd detection and management using cascade classifier on ARMv8 and OpenCV-Python.\n",
      "The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street during any festive occasions. The intelligent monitoring technology has been developing in recent years and human tracking has made a lot of progress. In this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene. In our study, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python. A Haar cascade classifier is trained for human head detection. Human tracking is achieved by indicating the direction of movement of the person. The results of the analysis will be helpful in managing the crowd in any area with high density of crowds.\n",
      "Self-Adaptive Skin Segmentation in Color Images.\n",
      "In this paper, we present a new method for skin detection and segmentation, relying on spatial analysis of skin-tone pixels. Our contribution lies in introducing self-adaptive seeds, from which the skin probability is propagated using the distance transform. The seeds are determined from a local skin color model that is learned on-line from a presented image, without requiring any additional information. This is in contrast to the existing methods that need a skin sample for the adaptation, e.g., acquired using a face detector. In our experimental study, we obtained F-score of over 0.85 for the ECU benchmark, and this is highly competitive compared with several state-of-the-art methods.\n",
      "Learning to guide task and motion planning using score-space representation.\n",
      "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems. Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another. We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score space, where we represent a problem instance in terms of performance of a set of solutions attempted so far. Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space. We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems. Results indicate that our approach perform orders of magnitudes faster than an unguided planner.\n",
      "Can machine learning aid in delivering new use cases and scenarios in 5G?.\n",
      "5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios. These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency. Consequently, novel techniques and strategies are required to address these challenges in a smarter way. In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective. The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges. It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.\n",
      "ANALYSIS OF VITAL SIGNS MONITORING USING AN IR-UWB RADAR.\n",
      "Ultra-wide Band (UWB) technology is a new, useful and safe technology in the field of wireless body networks. This paper focuses on the feasibility of estimating vital signs — specifically breathing rate and heartbeat frequency — from the spectrum of recorded waveforms, using an impulse-radio (IR) UWB radar. To this end, an analytical model is developed to perform and interpret the spectral analysis. Both the harmonics and the intermodulation between respiration and heart signals are addressed. Simulations have been performed to demonstrate how they affect the detection of vital signs and also to analyze the influence of the pulse waveform. A filter to cancel out breathing harmonics is also proposed to improve heart rate detection. The results of the experiments are presented under different scenarios which demonstrate the accuracy of the proposed technique for determining respiration and heartbeat rates. It has been shown that an IR-UWB radar can meet the requirements of typical biomedical applications such as non-invasive heart and respiration rate monitoring.\n",
      "SQL-to-NoSQL Schema Denormalization and Migration: A Study on Content Management Systems.\n",
      "Content management systems (CMSs) are able to let people, who have no technical skill to manage websites, rapidly create, edit, organize and publish online contents. For example, CMSs are often used to establish e-commerce online shops, community portals, personal blogs, or organization websites. Thus, CMSs play the role of collecting data and then can be easily extended to become intelligent Internet systems. Most popular CMSs, e.g., Word Press and Joomla, rely on relational databases or SQL databases, e.g., MySQL, PostgreSQL, etc. However, due to the explosive growth of huge data, the well-structured characteristic of SQL databases may limit the scalability to handle horizontal scaling. Therefore, regarding the flexibility and the feasibility of parallel processing, cloud computing takes Not Only SQL (NoSQL) databases into consideration. How to migrate the original CMS software from SQL databases to NoSQL databases becomes one emerging and critical research issue. This paper is motivated to propose an autonomous SQL-to-NoSQL schema denormalization and migration. At the same time, this paper also evaluates the proposed mechanism on the realistic CMS software. Based on our experimental results, our mechanism not only helps the current CMS software migrate into the cloud platform without re-designing their database schemas, but also improves at least 45% access performance after schema migration.\n",
      "Exploiting the Data Redundancy Locality to Improve the Performance of Deduplication-Based Storage Systems.\n",
      "The chunk-lookup disk bottleneck and the read amplification problems are two great challenges for deduplication-based storage systems and restrict the applicability of data deduplication for large-scale data volumes. Previous studies and our experimental evaluations have shown that the amount of redundant data shared among different types of applications is negligible. Based on the observations, we propose AA-Plus which effectively groups the hash index of the same application together and divides the whole hash index into different groups based on the application types. Moreover, it groups the data chunks of the same application together on the disks. The extensive trace-driven experiments conducted on our lightweight prototype implementation of AA-Plus show that compared with AA-Dedupe, AA-Plus significantly speeds up the write throughput by a factor of up to 6.9 and with an average of 3.1, and speeds up the read throughput by a factor of up to 3.3 and with an average of 1.9.\n",
      "Robot-assisted movement training compared with conventional therapy techniques for the rehabilitation of upper-limb motor function after stroke..\n",
      "OBJECTIVE\n",
      "To compare the effects of robot-assisted movement training with conventional techniques for the rehabilitation of upper-limb motor function after stroke.\n",
      "\n",
      "\n",
      "DESIGN\n",
      "Randomized controlled trial, 6-month follow-up.\n",
      "\n",
      "\n",
      "SETTING\n",
      "A Department of Veterans Affairs rehabilitation research and development center.\n",
      "\n",
      "\n",
      "PARTICIPANTS\n",
      "Consecutive sample of 27 subjects with chronic hemiparesis (>6mo after cerebrovascular accident) randomly allocated to group.\n",
      "\n",
      "\n",
      "INTERVENTIONS\n",
      "All subjects received twenty-four 1-hour sessions over 2 months. Subjects in the robot group practiced shoulder and elbow movements while assisted by a robot manipulator. Subjects in the control group received neurodevelopmental therapy (targeting proximal upper limb function) and 5 minutes of exposure to the robot in each session.\n",
      "\n",
      "\n",
      "MAIN OUTCOME MEASURES\n",
      "Fugl-Meyer assessment of motor impairment, FIMtrade mark instrument, and biomechanic measures of strength and reaching kinematics. Clinical evaluations were performed by a therapist blinded to group assignments.\n",
      "\n",
      "\n",
      "RESULTS\n",
      "Compared with the control group, the robot group had larger improvements in the proximal movement portion of the Fugl-Meyer test after 1 month of treatment (P<.05) and also after 2 months of treatment (P<.05). The robot group had larger gains in strength (P<.02) and larger increases in reach extent (P<.01) after 2 months of treatment. At the 6-month follow-up, the groups no longer differed in terms of the Fugl-Meyer test (P>.30); however, the robot group had larger improvements in the FIM (P<.04).\n",
      "\n",
      "\n",
      "CONCLUSIONS\n",
      "Compared with conventional treatment, robot-assisted movements had advantages in terms of clinical and biomechanical measures. Further research into the use of robotic manipulation for motor rehabilitation is justified.\n",
      "Interaction in the segmentation of medical images: A survey.\n",
      "Segmentation of the object of interest is a difficult step in the analysis of digital images. Fully automatic methods sometimes fail, producing incorrect results and requiring the intervention of a human operator. This is often true in medical applications, where image segmentation is particularly difficult due to restrictions imposed by image acquisition, pathology and biological variation. In this paper we present an early review of the largely unknown territory of human-computer interaction in image segmentation. The purpose is to identify patterns in the use of interaction and to develop qualitative criteria to evaluate interactive segmentation methods. We discuss existing interactive methods with respect to the following aspects: the type of information provided by the user, how this information affects the computational part, and the purpose of interaction in the segmentation process. The discussion is based on the potential impact of each strategy on the accuracy, repeatability and interaction efficiency. Among others, these are important aspects to characterise and understand the implications of interaction to the results generated by an interactive segmentation method. This survey is focused on medical imaging, however similar patterns are expected to hold for other applications as well.\n",
      "Deep Boltzmann Machines.\n",
      "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus, _, _ = load_data(\"scidocs\")\n",
    "key = random.choice(list(corpus.keys()))\n",
    "doc_beir = corpus[key]['text']\n",
    "for line in doc_beir.split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Exploiting the Data Redundancy Locality to Improve the Performance of Deduplication-Based Storage Systems.\n",
      "The chunk-lookup disk bottleneck and the read amplification problems are two great challenges for deduplication-based storage systems and restrict the applicability of data deduplication for large-scale data volumes.\n",
      "Previous studies and our experimental evaluations have shown that the amount of redundant data shared among different types of applications is negligible.\n",
      "Based on the observations, we propose AA-Plus which effectively groups the hash index of the same application together and divides the whole hash index into different groups based on the application types.\n",
      "Moreover, it groups the data chunks of the same application together on the disks.\n",
      "The extensive trace-driven experiments conducted on our lightweight prototype implementation of AA-Plus show that compared with AA-Dedupe, AA-Plus significantly speeds up the write throughput by a factor of up to 6.9 and with an average of 3.1, and speeds up the read throughput by a factor of up to 3.3 and with an average of 1.9.\n",
      "\n",
      "doc0|2 Crowd detection and management using cascade classifier on ARMv8 and OpenCV-Python.\n",
      "The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street during any festive occasions.\n",
      "The intelligent monitoring technology has been developing in recent years and human tracking has made a lot of progress.\n",
      "In this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene.\n",
      "In our study, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python.\n",
      "A Haar cascade classifier is trained for human head detection.\n",
      "Human tracking is achieved by indicating the direction of movement of the person.\n",
      "The results of the analysis will be helpful in managing the crowd in any area with high density of crowds.\n",
      "Self-Adaptive Skin Segmentation in Color Images.\n",
      "In this paper, we present a new method for skin detection and segmentation, relying on spatial analysis of skin-tone pixels.\n",
      "Our contribution lies in introducing self-adaptive seeds, from which the skin probability is propagated using the distance transform.\n",
      "The seeds are determined from a local skin color model that is learned on-line from a presented image, without requiring any additional information.\n",
      "This is in contrast to the existing methods that need a skin sample for the adaptation, e.g., acquired using a face detector.\n",
      "In our experimental study, we obtained F-score of over 0.85 for the ECU benchmark, and this is highly competitive compared with several state-of-the-art methods.\n",
      "Learning to guide task and motion planning using score-space representation.\n",
      "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems.\n",
      "Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another.\n",
      "We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score space, where we represent a problem instance in terms of performance of a set of solutions attempted so far.\n",
      "Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space.\n",
      "We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems.\n",
      "Results indicate that our approach perform orders of magnitudes faster than an unguided planner.\n",
      "Can machine learning aid in delivering new use cases and scenarios in 5G?.\n",
      "5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios.\n",
      "These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency.\n",
      "Consequently, novel techniques and strategies are required to address these challenges in a smarter way.\n",
      "In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective.\n",
      "The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges.\n",
      "It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.\n",
      "ANALYSIS OF VITAL SIGNS MONITORING USING AN IR-UWB RADAR.\n",
      "Ultra-wide Band (UWB) technology is a new, useful and safe technology in the field of wireless body networks.\n",
      "This paper focuses on the feasibility of estimating vital signs — specifically breathing rate and heartbeat frequency — from the spectrum of recorded waveforms, using an impulse-radio (IR) UWB radar.\n",
      "Both the harmonics and the intermodulation between respiration and heart signals are addressed.\n",
      "Simulations have been performed to demonstrate how they affect the detection of vital signs and also to analyze the influence of the pulse waveform.\n",
      "A filter to cancel out breathing harmonics is also proposed to improve heart rate detection.\n",
      "The results of the experiments are presented under different scenarios which demonstrate the accuracy of the proposed technique for determining respiration and heartbeat rates.\n",
      "It has been shown that an IR-UWB radar can meet the requirements of typical biomedical applications such as non-invasive heart and respiration rate monitoring.\n",
      "SQL-to-NoSQL Schema Denormalization and Migration: A Study on Content Management Systems.\n",
      "Content management systems (CMSs) are able to let people, who have no technical skill to manage websites, rapidly create, edit, organize and publish online contents.\n",
      "For example, CMSs are often used to establish e-commerce online shops, community portals, personal blogs, or organization websites.\n",
      "Thus, CMSs play the role of collecting data and then can be easily extended to become intelligent Internet systems.\n",
      "Most popular CMSs, e.g., Word Press and Joomla, rely on relational databases or SQL databases, e.g., MySQL, PostgreSQL, etc.\n",
      "However, due to the explosive growth of huge data, the well-structured characteristic of SQL databases may limit the scalability to handle horizontal scaling.\n",
      "Therefore, regarding the flexibility and the feasibility of parallel processing, cloud computing takes Not Only SQL (NoSQL) databases into consideration.\n",
      "How to migrate the original CMS software from SQL databases to NoSQL databases becomes one emerging and critical research issue.\n",
      "This paper is motivated to propose an autonomous SQL-to-NoSQL schema denormalization and migration.\n",
      "At the same time, this paper also evaluates the proposed mechanism on the realistic CMS software.\n",
      "Based on our experimental results, our mechanism not only helps the current CMS software migrate into the cloud platform without re-designing their database schemas, but also improves at least 45% access performance after schema migration.\n",
      "Robot-assisted movement training compared with conventional therapy techniques for the rehabilitation of upper-limb motor function after stroke..\n",
      "To compare the effects of robot-assisted movement training with conventional techniques for the rehabilitation of upper-limb motor function after stroke.\n",
      "Randomized controlled trial, 6-month follow-up.\n",
      "PARTICIPANTS\n",
      "Consecutive sample of 27 subjects with chronic hemiparesis (>6mo after cerebrovascular accident) randomly allocated to group.\n",
      "All subjects received twenty-four 1-hour sessions over 2 months.\n",
      "Subjects in the robot group practiced shoulder and elbow movements while assisted by a robot manipulator.\n",
      "Subjects in the control group received neurodevelopmental therapy (targeting proximal upper limb function) and 5 minutes of exposure to the robot in each session.\n",
      "Fugl-Meyer assessment of motor impairment, FIMtrade mark instrument, and biomechanic measures of strength and reaching kinematics.\n",
      "Clinical evaluations were performed by a therapist blinded to group assignments.\n",
      "RESULTS\n",
      "Compared with the control group, the robot group had larger improvements in the proximal movement portion of the Fugl-Meyer test after 1 month of treatment (P<.05) and also after 2 months of treatment (P<.05).\n",
      "The robot group had larger gains in strength (P<.02) and larger increases in reach extent (P<.01) after 2 months of treatment.\n",
      "At the 6-month follow-up, the groups no longer differed in terms of the Fugl-Meyer test (P>.30); however, the robot group had larger improvements in the FIM (P<.04).\n",
      "CONCLUSIONS\n",
      "Compared with conventional treatment, robot-assisted movements had advantages in terms of clinical and biomechanical measures.\n",
      "Further research into the use of robotic manipulation for motor rehabilitation is justified.\n",
      "Interaction in the segmentation of medical images: A survey.\n",
      "Segmentation of the object of interest is a difficult step in the analysis of digital images.\n",
      "Fully automatic methods sometimes fail, producing incorrect results and requiring the intervention of a human operator.\n",
      "This is often true in medical applications, where image segmentation is particularly difficult due to restrictions imposed by image acquisition, pathology and biological variation.\n",
      "In this paper we present an early review of the largely unknown territory of human-computer interaction in image segmentation.\n",
      "The purpose is to identify patterns in the use of interaction and to develop qualitative criteria to evaluate interactive segmentation methods.\n",
      "We discuss existing interactive methods with respect to the following aspects: the type of information provided by the user, how this information affects the computational part, and the purpose of interaction in the segmentation process.\n",
      "The discussion is based on the potential impact of each strategy on the accuracy, repeatability and interaction efficiency.\n",
      "Among others, these are important aspects to characterise and understand the implications of interaction to the results generated by an interactive segmentation method.\n",
      "This survey is focused on medical imaging, however similar patterns are expected to hold for other applications as well.\n",
      "Deep Boltzmann Machines.\n",
      "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables.\n",
      "The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters.\n",
      "The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass.\n",
      "We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n",
      "\n",
      "doc0|3 OBJECTIVE\n",
      "\n",
      "doc0|4 A Department of Veterans Affairs rehabilitation research and development center.\n",
      "\n",
      "doc0|5 To this end, an analytical model is developed to perform and interpret the spectral analysis.\n",
      "\n",
      "doc0|6 MAIN OUTCOME MEASURES\n",
      "\n",
      "doc0|7 INTERVENTIONS\n",
      "\n",
      "doc0|8 DESIGN\n",
      "\n",
      "doc0|9 Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains.\n",
      "\n",
      "doc0|10 SETTING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_split(doc_beir, doc_id=\"doc0\", n_clusters=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Crowd detection and management using cascade classifier on ARMv8 and OpenCV-Python.\n",
      "The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street during any festive occasions.\n",
      "The intelligent monitoring technology has been developing in recent years and human tracking has made a lot of progress.\n",
      "In this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene.\n",
      "In our study, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python.\n",
      "A Haar cascade classifier is trained for human head detection.\n",
      "Human tracking is achieved by indicating the direction of movement of the person.\n",
      "The results of the analysis will be helpful in managing the crowd in any area with high density of crowds.\n",
      "Self-Adaptive Skin Segmentation in Color Images.\n",
      "In this paper, we present a new method for skin detection and segmentation, relying on spatial analysis of skin-tone pixels.\n",
      "Our contribution lies in introducing self-adaptive seeds, from which the skin probability is propagated using the distance transform.\n",
      "The seeds are determined from a local skin color model that is learned on-line from a presented image, without requiring any additional information.\n",
      "This is in contrast to the existing methods that need a skin sample for the adaptation, e.g., acquired using a face detector.\n",
      "In our experimental study, we obtained F-score of over 0.85 for the ECU benchmark, and this is highly competitive compared with several state-of-the-art methods.\n",
      "Learning to guide task and motion planning using score-space representation.\n",
      "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems.\n",
      "Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another.\n",
      "We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score space, where we represent a problem instance in terms of performance of a set of solutions attempted so far.\n",
      "Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space.\n",
      "We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems.\n",
      "Results indicate that our approach perform orders of magnitudes faster than an unguided planner.\n",
      "Can machine learning aid in delivering new use cases and scenarios in 5G?.\n",
      "5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios.\n",
      "These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency.\n",
      "Consequently, novel techniques and strategies are required to address these challenges in a smarter way.\n",
      "In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective.\n",
      "The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges.\n",
      "It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.\n",
      "ANALYSIS OF VITAL SIGNS MONITORING USING AN IR-UWB RADAR.\n",
      "Ultra-wide Band (UWB) technology is a new, useful and safe technology in the field of wireless body networks.\n",
      "This paper focuses on the feasibility of estimating vital signs — specifically breathing rate and heartbeat frequency — from the spectrum of recorded waveforms, using an impulse-radio (IR) UWB radar.\n",
      "To this end, an analytical model is developed to perform and interpret the spectral analysis.\n",
      "Both the harmonics and the intermodulation between respiration and heart signals are addressed.\n",
      "Simulations have been performed to demonstrate how they affect the detection of vital signs and also to analyze the influence of the pulse waveform.\n",
      "A filter to cancel out breathing harmonics is also proposed to improve heart rate detection.\n",
      "The results of the experiments are presented under different scenarios which demonstrate the accuracy of the proposed technique for determining respiration and heartbeat rates.\n",
      "It has been shown that an IR-UWB radar can meet the requirements of typical biomedical applications such as non-invasive heart and respiration rate monitoring.\n",
      "SQL-to-NoSQL Schema Denormalization and Migration: A Study on Content Management Systems.\n",
      "Content management systems (CMSs) are able to let people, who have no technical skill to manage websites, rapidly create, edit, organize and publish online contents.\n",
      "For example, CMSs are often used to establish e-commerce online shops, community portals, personal blogs, or organization websites.\n",
      "Thus, CMSs play the role of collecting data and then can be easily extended to become intelligent Internet systems.\n",
      "Most popular CMSs, e.g., Word Press and Joomla, rely on relational databases or SQL databases, e.g., MySQL, PostgreSQL, etc.\n",
      "However, due to the explosive growth of huge data, the well-structured characteristic of SQL databases may limit the scalability to handle horizontal scaling.\n",
      "Therefore, regarding the flexibility and the feasibility of parallel processing, cloud computing takes Not Only SQL (NoSQL) databases into consideration.\n",
      "How to migrate the original CMS software from SQL databases to NoSQL databases becomes one emerging and critical research issue.\n",
      "This paper is motivated to propose an autonomous SQL-to-NoSQL schema denormalization and migration.\n",
      "At the same time, this paper also evaluates the proposed mechanism on the realistic CMS software.\n",
      "Based on our experimental results, our mechanism not only helps the current CMS software migrate into the cloud platform without re-designing their database schemas, but also improves at least 45% access performance after schema migration.\n",
      "Exploiting the Data Redundancy Locality to Improve the Performance of Deduplication-Based Storage Systems.\n",
      "The chunk-lookup disk bottleneck and the read amplification problems are two great challenges for deduplication-based storage systems and restrict the applicability of data deduplication for large-scale data volumes.\n",
      "Previous studies and our experimental evaluations have shown that the amount of redundant data shared among different types of applications is negligible.\n",
      "Based on the observations, we propose AA-Plus which effectively groups the hash index of the same application together and divides the whole hash index into different groups based on the application types.\n",
      "Moreover, it groups the data chunks of the same application together on the disks.\n",
      "The extensive trace-driven experiments conducted on our lightweight prototype implementation of AA-Plus show that compared with AA-Dedupe, AA-Plus significantly speeds up the write throughput by a factor of up to 6.9 and with an average of 3.1, and speeds up the read throughput by a factor of up to 3.3 and with an average of 1.9.\n",
      "Robot-assisted movement training compared with conventional therapy techniques for the rehabilitation of upper-limb motor function after stroke..\n",
      "OBJECTIVE\n",
      "To compare the effects of robot-assisted movement training with conventional techniques for the rehabilitation of upper-limb motor function after stroke.\n",
      "DESIGN\n",
      "Randomized controlled trial, 6-month follow-up.\n",
      "SETTING\n",
      "A Department of Veterans Affairs rehabilitation research and development center.\n",
      "PARTICIPANTS\n",
      "Consecutive sample of 27 subjects with chronic hemiparesis (>6mo after cerebrovascular accident) randomly allocated to group.\n",
      "INTERVENTIONS\n",
      "All subjects received twenty-four 1-hour sessions over 2 months.\n",
      "Subjects in the robot group practiced shoulder and elbow movements while assisted by a robot manipulator.\n",
      "Subjects in the control group received neurodevelopmental therapy (targeting proximal upper limb function) and 5 minutes of exposure to the robot in each session.\n",
      "MAIN OUTCOME MEASURES\n",
      "Fugl-Meyer assessment of motor impairment, FIMtrade mark instrument, and biomechanic measures of strength and reaching kinematics.\n",
      "Clinical evaluations were performed by a therapist blinded to group assignments.\n",
      "RESULTS\n",
      "Compared with the control group, the robot group had larger improvements in the proximal movement portion of the Fugl-Meyer test after 1 month of treatment (P<.05) and also after 2 months of treatment (P<.05).\n",
      "The robot group had larger gains in strength (P<.02) and larger increases in reach extent (P<.01) after 2 months of treatment.\n",
      "At the 6-month follow-up, the groups no longer differed in terms of the Fugl-Meyer test (P>.30); however, the robot group had larger improvements in the FIM (P<.04).\n",
      "CONCLUSIONS\n",
      "Compared with conventional treatment, robot-assisted movements had advantages in terms of clinical and biomechanical measures.\n",
      "Further research into the use of robotic manipulation for motor rehabilitation is justified.\n",
      "Interaction in the segmentation of medical images: A survey.\n",
      "Segmentation of the object of interest is a difficult step in the analysis of digital images.\n",
      "Fully automatic methods sometimes fail, producing incorrect results and requiring the intervention of a human operator.\n",
      "This is often true in medical applications, where image segmentation is particularly difficult due to restrictions imposed by image acquisition, pathology and biological variation.\n",
      "In this paper we present an early review of the largely unknown territory of human-computer interaction in image segmentation.\n",
      "The purpose is to identify patterns in the use of interaction and to develop qualitative criteria to evaluate interactive segmentation methods.\n",
      "We discuss existing interactive methods with respect to the following aspects: the type of information provided by the user, how this information affects the computational part, and the purpose of interaction in the segmentation process.\n",
      "The discussion is based on the potential impact of each strategy on the accuracy, repeatability and interaction efficiency.\n",
      "Among others, these are important aspects to characterise and understand the implications of interaction to the results generated by an interactive segmentation method.\n",
      "This survey is focused on medical imaging, however similar patterns are expected to hold for other applications as well.\n",
      "Deep Boltzmann Machines.\n",
      "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables.\n",
      "Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains.\n",
      "The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters.\n",
      "The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass.\n",
      "We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_preserve(doc_beir, doc_id=\"doc0\", min_samples=10)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constrained single-linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|0 Crowd detection and management using cascade classifier on ARMv8 and OpenCV-Python.\n",
      "The intelligent monitoring technology has been developing in recent years and human tracking has made a lot of progress.\n",
      "In our study, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python.\n",
      "A Haar cascade classifier is trained for human head detection.\n",
      "Human tracking is achieved by indicating the direction of movement of the person.\n",
      "\n",
      "doc0|1 The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street during any festive occasions.\n",
      "In this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene.\n",
      "The results of the analysis will be helpful in managing the crowd in any area with high density of crowds.\n",
      "OBJECTIVE\n",
      "PARTICIPANTS\n",
      "INTERVENTIONS\n",
      "MAIN OUTCOME MEASURES\n",
      "RESULTS\n",
      "CONCLUSIONS\n",
      "\n",
      "doc0|2 Self-Adaptive Skin Segmentation in Color Images.\n",
      "In this paper, we present a new method for skin detection and segmentation, relying on spatial analysis of skin-tone pixels.\n",
      "Our contribution lies in introducing self-adaptive seeds, from which the skin probability is propagated using the distance transform.\n",
      "The seeds are determined from a local skin color model that is learned on-line from a presented image, without requiring any additional information.\n",
      "This is in contrast to the existing methods that need a skin sample for the adaptation, e.g., acquired using a face detector.\n",
      "To this end, an analytical model is developed to perform and interpret the spectral analysis.\n",
      "DESIGN\n",
      "SETTING\n",
      "\n",
      "doc0|3 In our experimental study, we obtained F-score of over 0.85 for the ECU benchmark, and this is highly competitive compared with several state-of-the-art methods.\n",
      "Deep Boltzmann Machines.\n",
      "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables.\n",
      "Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains.\n",
      "The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters.\n",
      "The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass.\n",
      "We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n",
      "\n",
      "doc0|4 Learning to guide task and motion planning using score-space representation.\n",
      "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems.\n",
      "Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another.\n",
      "We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score space, where we represent a problem instance in terms of performance of a set of solutions attempted so far.\n",
      "Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space.\n",
      "We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems.\n",
      "Results indicate that our approach perform orders of magnitudes faster than an unguided planner.\n",
      "Fully automatic methods sometimes fail, producing incorrect results and requiring the intervention of a human operator.\n",
      "\n",
      "doc0|5 Can machine learning aid in delivering new use cases and scenarios in 5G?.\n",
      "5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios.\n",
      "These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency.\n",
      "Consequently, novel techniques and strategies are required to address these challenges in a smarter way.\n",
      "In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective.\n",
      "The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges.\n",
      "It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.\n",
      "However, due to the explosive growth of huge data, the well-structured characteristic of SQL databases may limit the scalability to handle horizontal scaling.\n",
      "Therefore, regarding the flexibility and the feasibility of parallel processing, cloud computing takes Not Only SQL (NoSQL) databases into consideration.\n",
      "\n",
      "doc0|6 ANALYSIS OF VITAL SIGNS MONITORING USING AN IR-UWB RADAR.\n",
      "Ultra-wide Band (UWB) technology is a new, useful and safe technology in the field of wireless body networks.\n",
      "This paper focuses on the feasibility of estimating vital signs — specifically breathing rate and heartbeat frequency — from the spectrum of recorded waveforms, using an impulse-radio (IR) UWB radar.\n",
      "Both the harmonics and the intermodulation between respiration and heart signals are addressed.\n",
      "Simulations have been performed to demonstrate how they affect the detection of vital signs and also to analyze the influence of the pulse waveform.\n",
      "A filter to cancel out breathing harmonics is also proposed to improve heart rate detection.\n",
      "The results of the experiments are presented under different scenarios which demonstrate the accuracy of the proposed technique for determining respiration and heartbeat rates.\n",
      "It has been shown that an IR-UWB radar can meet the requirements of typical biomedical applications such as non-invasive heart and respiration rate monitoring.\n",
      "Fugl-Meyer assessment of motor impairment, FIMtrade mark instrument, and biomechanic measures of strength and reaching kinematics.\n",
      "\n",
      "doc0|7 SQL-to-NoSQL Schema Denormalization and Migration: A Study on Content Management Systems.\n",
      "Content management systems (CMSs) are able to let people, who have no technical skill to manage websites, rapidly create, edit, organize and publish online contents.\n",
      "For example, CMSs are often used to establish e-commerce online shops, community portals, personal blogs, or organization websites.\n",
      "Thus, CMSs play the role of collecting data and then can be easily extended to become intelligent Internet systems.\n",
      "Most popular CMSs, e.g., Word Press and Joomla, rely on relational databases or SQL databases, e.g., MySQL, PostgreSQL, etc.\n",
      "How to migrate the original CMS software from SQL databases to NoSQL databases becomes one emerging and critical research issue.\n",
      "This paper is motivated to propose an autonomous SQL-to-NoSQL schema denormalization and migration.\n",
      "At the same time, this paper also evaluates the proposed mechanism on the realistic CMS software.\n",
      "Based on our experimental results, our mechanism not only helps the current CMS software migrate into the cloud platform without re-designing their database schemas, but also improves at least 45% access performance after schema migration.\n",
      "\n",
      "doc0|8 Exploiting the Data Redundancy Locality to Improve the Performance of Deduplication-Based Storage Systems.\n",
      "The chunk-lookup disk bottleneck and the read amplification problems are two great challenges for deduplication-based storage systems and restrict the applicability of data deduplication for large-scale data volumes.\n",
      "Previous studies and our experimental evaluations have shown that the amount of redundant data shared among different types of applications is negligible.\n",
      "Based on the observations, we propose AA-Plus which effectively groups the hash index of the same application together and divides the whole hash index into different groups based on the application types.\n",
      "Moreover, it groups the data chunks of the same application together on the disks.\n",
      "The extensive trace-driven experiments conducted on our lightweight prototype implementation of AA-Plus show that compared with AA-Dedupe, AA-Plus significantly speeds up the write throughput by a factor of up to 6.9 and with an average of 3.1, and speeds up the read throughput by a factor of up to 3.3 and with an average of 1.9.\n",
      "\n",
      "doc0|9 Robot-assisted movement training compared with conventional therapy techniques for the rehabilitation of upper-limb motor function after stroke..\n",
      "To compare the effects of robot-assisted movement training with conventional techniques for the rehabilitation of upper-limb motor function after stroke.\n",
      "Subjects in the robot group practiced shoulder and elbow movements while assisted by a robot manipulator.\n",
      "Subjects in the control group received neurodevelopmental therapy (targeting proximal upper limb function) and 5 minutes of exposure to the robot in each session.\n",
      "Compared with the control group, the robot group had larger improvements in the proximal movement portion of the Fugl-Meyer test after 1 month of treatment (P<.05) and also after 2 months of treatment (P<.05).\n",
      "The robot group had larger gains in strength (P<.02) and larger increases in reach extent (P<.01) after 2 months of treatment.\n",
      "At the 6-month follow-up, the groups no longer differed in terms of the Fugl-Meyer test (P>.30); however, the robot group had larger improvements in the FIM (P<.04).\n",
      "Compared with conventional treatment, robot-assisted movements had advantages in terms of clinical and biomechanical measures.\n",
      "Further research into the use of robotic manipulation for motor rehabilitation is justified.\n",
      "\n",
      "doc0|10 Randomized controlled trial, 6-month follow-up.\n",
      "A Department of Veterans Affairs rehabilitation research and development center.\n",
      "Consecutive sample of 27 subjects with chronic hemiparesis (>6mo after cerebrovascular accident) randomly allocated to group.\n",
      "All subjects received twenty-four 1-hour sessions over 2 months.\n",
      "Clinical evaluations were performed by a therapist blinded to group assignments.\n",
      "\n",
      "doc0|11 Interaction in the segmentation of medical images: A survey.\n",
      "Segmentation of the object of interest is a difficult step in the analysis of digital images.\n",
      "This is often true in medical applications, where image segmentation is particularly difficult due to restrictions imposed by image acquisition, pathology and biological variation.\n",
      "In this paper we present an early review of the largely unknown territory of human-computer interaction in image segmentation.\n",
      "The purpose is to identify patterns in the use of interaction and to develop qualitative criteria to evaluate interactive segmentation methods.\n",
      "We discuss existing interactive methods with respect to the following aspects: the type of information provided by the user, how this information affects the computational part, and the purpose of interaction in the segmentation process.\n",
      "The discussion is based on the potential impact of each strategy on the accuracy, repeatability and interaction efficiency.\n",
      "Among others, these are important aspects to characterise and understand the implications of interaction to the results generated by an interactive segmentation method.\n",
      "This survey is focused on medical imaging, however similar patterns are expected to hold for other applications as well.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker(doc_beir, doc_id=\"doc0\", encoder=get_encoder(\"huggingface\"))\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linkage` function from SciPy performs hierarchical clustering by linking two clusters of the shortest distance together at each step. However, this linkage strategy does not intrinsically take into account the size of each cluster. Let's take a look at this in depth with the \"Attention is All You Need\" example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 <-> 145\n",
      "8 <-> 156\n",
      "6 <-> 172\n",
      "28 <-> 165\n",
      "125 <-> 127\n",
      "141 <-> 174\n",
      "37 <-> 42\n",
      "94 <-> 98\n",
      "40 <-> 44\n",
      "50 <-> 59\n",
      "4 <-> 18\n",
      "32 <-> 33\n",
      "79 <-> 80\n",
      "110 <-> 111\n",
      "45 <-> 82\n",
      "66 <-> 67\n",
      "175 <-> 182\n",
      "147 <-> 148\n",
      "101 <-> 185\n",
      "81 <-> 87\n",
      "124 <-> 176\n",
      "62 <-> 181\n",
      "76 <-> 184\n",
      "100 <-> 102\n",
      "0 <-> 47\n",
      "117 <-> 119\n",
      "186 <-> 194\n",
      "74 <-> 169\n",
      "43 <-> 191\n",
      "178 <-> 200\n",
      "35 <-> 201\n",
      "126 <-> 192\n",
      "198 <-> 202\n",
      "25 <-> 195\n",
      "61 <-> 193\n",
      "26 <-> 205\n",
      "24 <-> 75\n",
      "65 <-> 69\n",
      "118 <-> 121\n",
      "48 <-> 53\n",
      "86 <-> 196\n",
      "106 <-> 108\n",
      "208 <-> 209\n",
      "77 <-> 204\n",
      "187 <-> 211\n",
      "36 <-> 215\n",
      "96 <-> 179\n",
      "190 <-> 217\n",
      "58 <-> 216\n",
      "112 <-> 207\n",
      "19 <-> 166\n",
      "180 <-> 219\n",
      "2 <-> 31\n",
      "10 <-> 27\n",
      "70 <-> 214\n",
      "167 <-> 177\n",
      "160 <-> 197\n",
      "3 <-> 223\n",
      "199 <-> 212\n",
      "16 <-> 17\n",
      "163 <-> 173\n",
      "170 <-> 232\n",
      "49 <-> 52\n",
      "105 <-> 109\n",
      "230 <-> 231\n",
      "89 <-> 92\n",
      "222 <-> 233\n",
      "220 <-> 236\n",
      "143 <-> 203\n",
      "135 <-> 218\n",
      "153 <-> 238\n",
      "78 <-> 229\n",
      "188 <-> 239\n",
      "224 <-> 243\n",
      "107 <-> 213\n",
      "221 <-> 245\n",
      "64 <-> 206\n",
      "210 <-> 228\n",
      "11 <-> 225\n",
      "12 <-> 34\n",
      "73 <-> 226\n",
      "93 <-> 237\n",
      "244 <-> 247\n",
      "71 <-> 252\n",
      "97 <-> 241\n",
      "183 <-> 254\n",
      "134 <-> 257\n",
      "85 <-> 258\n",
      "256 <-> 259\n",
      "23 <-> 246\n",
      "51 <-> 54\n",
      "120 <-> 249\n",
      "131 <-> 132\n",
      "250 <-> 260\n",
      "242 <-> 265\n",
      "154 <-> 189\n",
      "29 <-> 266\n",
      "144 <-> 168\n",
      "103 <-> 104\n",
      "30 <-> 152\n",
      "91 <-> 268\n",
      "255 <-> 272\n",
      "123 <-> 240\n",
      "227 <-> 263\n",
      "159 <-> 273\n",
      "248 <-> 276\n",
      "5 <-> 277\n",
      "56 <-> 278\n",
      "46 <-> 279\n",
      "161 <-> 275\n",
      "267 <-> 274\n",
      "280 <-> 281\n",
      "38 <-> 283\n",
      "130 <-> 282\n",
      "149 <-> 285\n",
      "146 <-> 284\n",
      "41 <-> 287\n",
      "39 <-> 288\n",
      "63 <-> 289\n",
      "234 <-> 262\n",
      "162 <-> 290\n",
      "235 <-> 270\n",
      "83 <-> 292\n",
      "15 <-> 171\n",
      "157 <-> 286\n",
      "261 <-> 293\n",
      "128 <-> 129\n",
      "90 <-> 294\n",
      "155 <-> 299\n",
      "158 <-> 300\n",
      "68 <-> 291\n",
      "95 <-> 251\n",
      "72 <-> 302\n",
      "115 <-> 264\n",
      "301 <-> 304\n",
      "60 <-> 306\n",
      "297 <-> 307\n",
      "296 <-> 305\n",
      "295 <-> 308\n",
      "269 <-> 310\n",
      "13 <-> 311\n",
      "22 <-> 312\n",
      "21 <-> 313\n",
      "142 <-> 271\n",
      "136 <-> 309\n",
      "314 <-> 316\n",
      "303 <-> 317\n",
      "253 <-> 318\n",
      "84 <-> 319\n",
      "140 <-> 320\n",
      "116 <-> 321\n",
      "55 <-> 322\n",
      "139 <-> 164\n",
      "315 <-> 323\n",
      "151 <-> 325\n",
      "1 <-> 9\n",
      "298 <-> 326\n",
      "133 <-> 328\n",
      "137 <-> 329\n",
      "88 <-> 330\n",
      "14 <-> 331\n",
      "114 <-> 332\n",
      "138 <-> 333\n",
      "113 <-> 334\n",
      "150 <-> 335\n",
      "20 <-> 336\n",
      "122 <-> 337\n",
      "324 <-> 327\n",
      "338 <-> 339\n",
      "99 <-> 340\n",
      "57 <-> 341\n"
     ]
    }
   ],
   "source": [
    "sents = split_sentences(doc_attention)\n",
    "embs = model.encode(sents)\n",
    "metric = partial(__dist__, n_segments=len(embs), lamda=0)\n",
    "distance_matrix = np.abs(pdist(embs, metric=metric))\n",
    "linkage_matrix = linkage(distance_matrix, method='single')\n",
    "\n",
    "for row in linkage_matrix:\n",
    "    print(f\"{int(row[0])} <-> {int(row[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the dendrogram from the `linkage` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dendrogram(linkage_matrix)\n",
    "plt.rcParams[\"figure.figsize\"] = (32, 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the blue color samples, they get left behind with this bottom-up approach. They are only clustered when the clustering of the green samples are finished. This becomes problematic when we call `fcluster` from scipy, which only has two main criteria for selecting clusters:\n",
    "- `distance`: A horizontal threshold line on the dendrogram. Because the blue samples are left behind, no matter where we set the threshold, you can clearly see these samples will become standalone because they only get grouped together later, not earlier.\n",
    "- `maxclust`: The number of clusters. `fcluster` only selects clusters AFTER the linkage matrix is already formed, instead of BEFORE. Therefore, it selects clusters to meet the `maxclust` requirement with a top-down approach. Because the blue samples are left behind, no matter how many clusters we set, they will be left out. For example, if we set `n_clusters=2`, then `57` becomes its own cluster, and all other samples are grouped together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc0|1 Attention Is All You Need\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism.\n",
      "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
      "On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences.\n",
      "Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
      "Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter.\n",
      "The fundamental constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n",
      "In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "In this work, we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S, all of which use convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions.\n",
      "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "This makes it more difficult to learn dependencies between distant positions.\n",
      "In the Transformer, this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "In the following sections, we will describe the Transformer, motivate self-attention, and discuss its advantages over models such as Extended Neural GPU, ByteNet, and ConvS2S.\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure.\n",
      "Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
      "Given the continuous representations, the decoder then generates an output sequence of symbols one element at a time.\n",
      "At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "Encoder and Decoder Stacks\n",
      "The encoder is composed of a stack of N = 6 identical layers.\n",
      "Each layer has two sub-layers.\n",
      "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
      "We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "The decoder is also composed of a stack of N = 6 identical layers.\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
      "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
      "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
      "This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
      "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "Scaled Dot-Product Attention\n",
      "The input consists of queries and keys of dimension dk, and values of dimension dv.\n",
      "We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\n",
      "The keys and values are also packed together into matrices K and V.\n",
      "We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\n",
      "The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\n",
      "Dot-product attention is identical to our algorithm, except for the scaling factor of 1/sqrt(dk).\n",
      "Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n",
      "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n",
      "We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
      "To counteract this effect, we scale the dot products by 1/sqrt(dk).\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively.\n",
      "On each of these projected versions of queries, keys, and values, we then perform the attention function in parallel, yielding dv-dimensional output values.\n",
      "These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "With a single attention head, averaging inhibits this.\n",
      "In this work, we employ h = 8 parallel attention layers, or heads.\n",
      "For each of these, we use dk = dv = dmodel/h = 64.\n",
      "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
      "This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
      "- The encoder contains self-attention layers.\n",
      "In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder.\n",
      "Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
      "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.\n",
      "We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\n",
      "Position-wise Feed-Forward Networks\n",
      "In addition to attention sub\n",
      "-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "This consists of two linear transformations with a ReLU activation in between.\n",
      "Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.\n",
      "In the embedding layers, we multiply those weights by sqrt(dmodel).\n",
      "Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.\n",
      "The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed.\n",
      "There are many choices of positional encodings, learned and fixed.\n",
      "In this work, we use sine and cosine functions of different frequencies.\n",
      "4 Why Self-Attention\n",
      "In this section, we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations to another sequence of equal length, such as a hidden layer in a typical sequence transduction encoder or decoder.\n",
      "Motivating our use of self-attention we consider three desiderata.\n",
      "One is the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
      "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.\n",
      "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.\n",
      "This would increase the maximum path length to O(n/r).\n",
      "We plan to investigate this approach further in future work.\n",
      "5 Training\n",
      "Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.\n",
      "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\n",
      "Sentence pairs were batched together by approximate sequence length.\n",
      "Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs.\n",
      "For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.\n",
      "We trained the base models for a total of 100,000 steps or 12 hours.\n",
      "For our big models, step time was 1.0 seconds.\n",
      "The big models were trained for 300,000 steps (3.5 days).\n",
      "Optimizer\n",
      "We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ϵ = 10^−9.\n",
      "We varied the learning rate over the course of training.\n",
      "Regularization\n",
      "We employ three types of regularization during training:\n",
      "- Residual Dropout:\n",
      "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
      "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "For the base model, we use a rate of Pdrop = 0.1.\n",
      "- Label Smoothing: During training, we employed label smoothing of value ϵls = 0.1.\n",
      "This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "The configuration of this model is listed in the bottom line of Table 3.\n",
      "Training took 3.5 days on 8 P100 GPUs.\n",
      "Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.\n",
      "The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.\n",
      "For the big models, we averaged the last 20 checkpoints.\n",
      "We used beam search with a beam size of 4 and length penalty α = 0.6.\n",
      "These hyperparameters were chosen after experimentation on the development set.\n",
      "We set the maximum output length during inference to input length + 50, but terminate early when possible.\n",
      "Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\n",
      "We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.\n",
      "This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
      "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, about 40K training sentences.\n",
      "We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences.\n",
      "We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "Our results show that despite the lack of task-specific tuning, our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar.\n",
      "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n",
      "In the former task, our best model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks.\n",
      "We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n",
      "Making generation less sequential is another research goal of ours.\n",
      "\n",
      "doc0|2 V\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, ids = cluster_chunker_k_split(doc_attention, doc_id=\"doc0\", n_clusters=2)\n",
    "log_clusters(texts, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for semantic chunking where the chunk size matters, we CANNOT rely on the `linkage` matrix in SciPy.\n",
    "\n",
    "My implementation is simple. We iterate through the distance matrix by looking at the minimal distance between 2 points at each step. If the sample size of the combination of the 2 clusters which each point belongs to is less than or equal to the `max_sample_per_cluster` threshold, then link them. Else, don't link them.\n",
    "\n",
    "This implementation ensures:\n",
    "1) A stable and balanced cluster size for all samples\n",
    "2) Still looking at the minimal distance at each step\n",
    "\n",
    "I cannot draw a dendrogram with my implementation because\n",
    "1) `linkage` creates new cluster IDs while my implementation uses sample IDs as cluster IDs.\n",
    "2) I do not create a linkage matrix.\n",
    "\n",
    "But you could obviously see the difference in the output chunks above. The dendrogram should have been evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dendrogram of the old clustering method on the sampled MLDR and BEIR document as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3200x1800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents = split_sentences(doc_mldr)\n",
    "embs = model.encode(sents)\n",
    "metric = partial(__dist__, n_segments=len(embs), lamda=0)\n",
    "distance_matrix = np.abs(pdist(embs, metric=metric))\n",
    "linkage_matrix = linkage(distance_matrix, method='single')\n",
    "dendrogram(linkage_matrix)\n",
    "plt.rcParams[\"figure.figsize\"] = (32, 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3200x1800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents = split_sentences(doc_beir)\n",
    "embs = model.encode(sents)\n",
    "metric = partial(__dist__, n_segments=len(embs), lamda=0)\n",
    "distance_matrix = np.abs(pdist(embs, metric=metric))\n",
    "linkage_matrix = linkage(distance_matrix, method='single')\n",
    "dendrogram(linkage_matrix)\n",
    "plt.rcParams[\"figure.figsize\"] = (32, 18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
